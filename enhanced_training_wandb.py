#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼ºè®­ç»ƒè„šæœ¬ - é›†æˆWandBå’Œæ”¹è¿›è¾“å‡ºç®¡ç†

æ–°å¢åŠŸèƒ½ï¼š
1. WandBå®éªŒè·Ÿè¸ªï¼ˆå¯é€‰ï¼‰
2. æ”¹è¿›çš„tqdmè¾“å‡ºç®¡ç†
3. æ›´æ¸…æ´çš„æ—¥å¿—è¾“å‡º
4. æ›´å¥½çš„subprocesså…¼å®¹æ€§

ä½¿ç”¨æ–¹æ³•:
python enhanced_training_wandb.py --config baseline --epochs 20
python enhanced_training_wandb.py --config baseline --epochs 20 --use_wandb  # ä½¿ç”¨WandB
python enhanced_training_wandb.py --config baseline --epochs 20 --quiet      # å®‰é™æ¨¡å¼

ä½œè€…: å‘¨ç‘å®¸
æ—¥æœŸ: 2025-05-25
"""

import argparse
import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader
import json
import logging
from datetime import datetime
from tqdm import tqdm
import warnings

# æ£€æŸ¥æ˜¯å¦åœ¨subprocessä¸­è¿è¡Œ
RUNNING_IN_SUBPROCESS = not sys.stdout.isatty()

# è®¾ç½®matplotlibåç«¯ï¼ˆé¿å…GUIé—®é¢˜ï¼‰
if RUNNING_IN_SUBPROCESS:
    matplotlib.use('Agg')
import matplotlib
matplotlib.use('Agg')

# å°†çˆ¶ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_training_multi_loss import EnhancedTrainingConfig
import enhanced_training_multi_loss

# å°è¯•å¯¼å…¥wandbï¼ˆå¯é€‰ï¼‰
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    warnings.warn("WandB not available. Install with: pip install wandb")

# è®¾ç½®æ—¥å¿—
def setup_logging(quiet=False):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    level = logging.WARNING if quiet else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger('enhanced_training_wandb')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class WandBTrainer:
    """é›†æˆWandBçš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, config, use_wandb=False, project_name="seafloor-topography"):
        self.model = model.to(enhanced_training_multi_loss.device)
        self.config = config
        self.use_wandb = use_wandb and WANDB_AVAILABLE
        
        # åˆå§‹åŒ–WandB
        if self.use_wandb:
            wandb.init(
                project=project_name,
                name=f"{config.LOSS_CONFIG['loss_combination']}_experiment",
                config=dict(config.LOSS_CONFIG),
                tags=[config.LOSS_CONFIG['loss_combination']],
                notes=f"Loss combination: {config.LOSS_CONFIG['loss_combination']}"
            )
            wandb.watch(model, log="all", log_freq=100)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = enhanced_training_multi_loss.MultiLossFunction(config.LOSS_CONFIG)
        self.optimizer = enhanced_training_multi_loss.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
            betas=(0.9, 0.999)
        )
        
        self.scheduler = enhanced_training_multi_loss.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=10,
            verbose=True,
            min_lr=1e-6
        )
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss_total': [],
            'train_loss_pixel': [],
            'train_loss_gradient': [],
            'train_loss_spectral': [],
            'val_loss_total': [],
            'val_loss_pixel': [],
            'val_loss_gradient': [],
            'val_loss_spectral': [],
            'val_correlation': [],
            'learning_rates': []
        }
        
        self.best_val_loss = float('inf')
        self.early_stopping_counter = 0
    
    def train_epoch(self, train_loader, quiet=False):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = {
            'total': 0.0,
            'pixel': 0.0,
            'gradient': 0.0,
            'spectral': 0.0
        }
        
        # è®¾ç½®è¿›åº¦æ¡
        if quiet or RUNNING_IN_SUBPROCESS:
            # åœ¨å®‰é™æ¨¡å¼æˆ–subprocessä¸­ä½¿ç”¨ç®€åŒ–çš„è¿›åº¦æ˜¾ç¤º
            progress_bar = train_loader
            desc = 'è®­ç»ƒä¸­'
        else:
            progress_bar = tqdm(train_loader, desc='è®­ç»ƒä¸­', leave=False)
        
        for batch_idx, batch_data in enumerate(progress_bar):
            
            if len(batch_data) == 3:
                features, labels, tid_weights = batch_data
                features = features.to(enhanced_training_multi_loss.device)
                labels = labels.to(enhanced_training_multi_loss.device)
                tid_weights = tid_weights.to(enhanced_training_multi_loss.device)
            else:
                features, labels = batch_data
                features = features.to(enhanced_training_multi_loss.device)
                labels = labels.to(enhanced_training_multi_loss.device)
                tid_weights = None

            # å‰å‘ä¼ æ’­
            outputs = self.model(features)
            loss, loss_components = self.criterion(outputs, labels, tid_weights)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()

            # è®°å½•æŸå¤±
            for key in epoch_losses:
                if key in loss_components:
                    epoch_losses[key] += loss_components[key]

            # æ›´æ–°è¿›åº¦æ¡ï¼ˆåªåœ¨éå®‰é™æ¨¡å¼ä¸‹ï¼‰
            if not quiet and not RUNNING_IN_SUBPROCESS and hasattr(progress_bar, 'set_postfix'):
                progress_bar.set_postfix({
                    'loss': f'{loss_components["total"]:.4f}',
                    'pixel': f'{loss_components["pixel"]:.4f}',
                    'grad': f'{loss_components["gradient"]:.4f}',
                    'spec': f'{loss_components["spectral"]:.4f}'
                })
            
            # WandBå®æ—¶æ—¥å¿—
            if self.use_wandb and batch_idx % 50 == 0:
                wandb.log({
                    'batch_loss': loss_components["total"],
                    'batch_pixel_loss': loss_components["pixel"],
                    'batch_gradient_loss': loss_components["gradient"],
                    'batch_spectral_loss': loss_components["spectral"],
                    'learning_rate': self.optimizer.param_groups[0]['lr']
                })

        n_batches = len(train_loader)
        return {key: val / n_batches for key, val in epoch_losses.items()}

    def validate(self, val_loader, quiet=False):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        epoch_losses = {
            'total': 0.0,
            'pixel': 0.0,
            'gradient': 0.0,
            'spectral': 0.0
        }

        all_predictions = []
        all_targets = []

        # è®¾ç½®è¿›åº¦æ¡
        if quiet or RUNNING_IN_SUBPROCESS:
            progress_bar = val_loader
        else:
            progress_bar = tqdm(val_loader, desc='éªŒè¯ä¸­', leave=False)

        with torch.no_grad():
            for batch_data in progress_bar:
                
                if len(batch_data) == 3:
                    features, labels, tid_weights = batch_data
                    features = features.to(enhanced_training_multi_loss.device)
                    labels = labels.to(enhanced_training_multi_loss.device)
                    tid_weights = tid_weights.to(enhanced_training_multi_loss.device)
                else:
                    features, labels = batch_data
                    features = features.to(enhanced_training_multi_loss.device)
                    labels = labels.to(enhanced_training_multi_loss.device)
                    tid_weights = None

                outputs = self.model(features)
                loss, loss_components = self.criterion(outputs, labels, tid_weights)

                # è®°å½•æŸå¤±
                for key in epoch_losses:
                    if key in loss_components:
                        epoch_losses[key] += loss_components[key]

                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions.append(outputs.cpu().numpy())
                all_targets.append(labels.cpu().numpy())

        # è®¡ç®—ç›¸å…³ç³»æ•°
        predictions = np.concatenate(all_predictions, axis=0)
        targets = np.concatenate(all_targets, axis=0)
        correlation = np.corrcoef(predictions.flatten(), targets.flatten())[0, 1]

        n_batches = len(val_loader)
        results = {key: val / n_batches for key, val in epoch_losses.items()}
        results['correlation'] = correlation

        return results

    def train(self, train_loader, val_loader, quiet=False):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger = logging.getLogger('enhanced_training_wandb')
        
        if not quiet:
            logger.info(f"å¼€å§‹è®­ç»ƒ - æŸå¤±å‡½æ•°ç»„åˆ: {self.config.LOSS_CONFIG['loss_combination']}")

        for epoch in range(self.config.NUM_EPOCHS):
            if not quiet:
                logger.info(f"Epoch {epoch + 1}/{self.config.NUM_EPOCHS}")

            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, quiet)

            # éªŒè¯
            val_metrics = self.validate(val_loader, quiet)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_metrics['total'])
            current_lr = self.optimizer.param_groups[0]['lr']

            # è®°å½•å†å²
            self.history['train_loss_total'].append(train_metrics['total'])
            self.history['train_loss_pixel'].append(train_metrics['pixel'])
            self.history['train_loss_gradient'].append(train_metrics['gradient'])
            self.history['train_loss_spectral'].append(train_metrics['spectral'])
            
            self.history['val_loss_total'].append(val_metrics['total'])
            self.history['val_loss_pixel'].append(val_metrics['pixel'])
            self.history['val_loss_gradient'].append(val_metrics['gradient'])
            self.history['val_loss_spectral'].append(val_metrics['spectral'])
            
            self.history['val_correlation'].append(val_metrics['correlation'])
            self.history['learning_rates'].append(current_lr)

            # WandBæ—¥å¿—
            if self.use_wandb:
                wandb.log({
                    'epoch': epoch + 1,
                    'train_loss_total': train_metrics['total'],
                    'train_loss_pixel': train_metrics['pixel'],
                    'train_loss_gradient': train_metrics['gradient'],
                    'train_loss_spectral': train_metrics['spectral'],
                    'val_loss_total': val_metrics['total'],
                    'val_loss_pixel': val_metrics['pixel'],
                    'val_loss_gradient': val_metrics['gradient'],
                    'val_loss_spectral': val_metrics['spectral'],
                    'val_correlation': val_metrics['correlation'],
                    'learning_rate': current_lr
                })

            # æ‰“å°æŒ‡æ ‡ï¼ˆåªåœ¨éå®‰é™æ¨¡å¼ä¸‹ï¼‰
            if not quiet:
                logger.info(f"è®­ç»ƒ - æ€»æŸå¤±: {train_metrics['total']:.6f}, åƒç´ : {train_metrics['pixel']:.6f}, "
                          f"æ¢¯åº¦: {train_metrics['gradient']:.6f}, é¢‘è°±: {train_metrics['spectral']:.6f}")
                logger.info(f"éªŒè¯ - æ€»æŸå¤±: {val_metrics['total']:.6f}, åƒç´ : {val_metrics['pixel']:.6f}, "
                          f"æ¢¯åº¦: {val_metrics['gradient']:.6f}, é¢‘è°±: {val_metrics['spectral']:.6f}, "
                          f"ç›¸å…³: {val_metrics['correlation']:.6f}")
            else:
                # å®‰é™æ¨¡å¼ä¸‹åªæ‰“å°å…³é”®ä¿¡æ¯
                print(f"Epoch {epoch + 1}/{self.config.NUM_EPOCHS}: "
                      f"Train: {train_metrics['total']:.4f}, "
                      f"Val: {val_metrics['total']:.4f}, "
                      f"Corr: {val_metrics['correlation']:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['total'] < self.best_val_loss:
                self.best_val_loss = val_metrics['total']
                self.early_stopping_counter = 0
                self.save_checkpoint(epoch, val_metrics['total'])
                if not quiet:
                    logger.info("ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                self.early_stopping_counter += 1

            # æ—©åœæ£€æŸ¥
            if self.early_stopping_counter >= self.config.EARLY_STOPPING_PATIENCE:
                if not quiet:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch + 1}")
                break

        if not quiet:
            logger.info("è®­ç»ƒå®Œæˆ!")
        
        # ç»“æŸWandBè¿è¡Œ
        if self.use_wandb:
            wandb.finish()

    def save_checkpoint(self, epoch, val_loss):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'config': self.config,
            'loss_config': self.config.LOSS_CONFIG,
            'history': self.history
        }

        path = os.path.join(self.config.OUTPUT_DIR, 
                           f'best_model_enhanced_multi_loss_{self.config.LOSS_CONFIG["loss_combination"]}_lt135km.pth')
        torch.save(checkpoint, path)


def create_training_visualization(trainer, config, output_dir):
    """åˆ›å»ºè®­ç»ƒå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    epochs = range(1, len(trainer.history['train_loss_total']) + 1)
    
    # æ€»æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, trainer.history['train_loss_total'], 'b-', label='è®­ç»ƒæ€»æŸå¤±', alpha=0.7)
    axes[0, 0].plot(epochs, trainer.history['val_loss_total'], 'r-', label='éªŒè¯æ€»æŸå¤±', alpha=0.7)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æ€»æŸå¤±')
    axes[0, 0].set_title('æ€»æŸå¤±å˜åŒ–')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # åƒç´ æŸå¤±æ›²çº¿
    axes[0, 1].plot(epochs, trainer.history['train_loss_pixel'], 'b-', label='è®­ç»ƒåƒç´ æŸå¤±', alpha=0.7)
    axes[0, 1].plot(epochs, trainer.history['val_loss_pixel'], 'r-', label='éªŒè¯åƒç´ æŸå¤±', alpha=0.7)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('åƒç´ æŸå¤±')
    axes[0, 1].set_title('åƒç´ æŸå¤±å˜åŒ–')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # æ¢¯åº¦æŸå¤±æ›²çº¿
    axes[0, 2].plot(epochs, trainer.history['train_loss_gradient'], 'b-', label='è®­ç»ƒæ¢¯åº¦æŸå¤±', alpha=0.7)
    axes[0, 2].plot(epochs, trainer.history['val_loss_gradient'], 'r-', label='éªŒè¯æ¢¯åº¦æŸå¤±', alpha=0.7)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('æ¢¯åº¦æŸå¤±')
    axes[0, 2].set_title('æ¢¯åº¦æŸå¤±å˜åŒ–')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # é¢‘è°±æŸå¤±æ›²çº¿
    axes[1, 0].plot(epochs, trainer.history['train_loss_spectral'], 'b-', label='è®­ç»ƒé¢‘è°±æŸå¤±', alpha=0.7)
    axes[1, 0].plot(epochs, trainer.history['val_loss_spectral'], 'r-', label='éªŒè¯é¢‘è°±æŸå¤±', alpha=0.7)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('é¢‘è°±æŸå¤±')
    axes[1, 0].set_title('é¢‘è°±æŸå¤±å˜åŒ–')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç›¸å…³ç³»æ•°
    axes[1, 1].plot(epochs, trainer.history['val_correlation'], 'g-', label='éªŒè¯ç›¸å…³ç³»æ•°', alpha=0.7)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('ç›¸å…³ç³»æ•°')
    axes[1, 1].set_title('ç›¸å…³ç³»æ•°å˜åŒ–')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡æ›²çº¿
    axes[1, 2].plot(epochs, trainer.history['learning_rates'], 'orange', alpha=0.7)
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('å­¦ä¹ ç‡')
    axes[1, 2].set_title('å­¦ä¹ ç‡å˜åŒ–')
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].set_yscale('log')
    
    plt.suptitle(f'è®­ç»ƒå†å² - {config.LOSS_CONFIG["loss_combination"]} æŸå¤±ç»„åˆ', fontsize=14)
    plt.tight_layout()
    
    plot_path = os.path.join(output_dir, f'training_history_{config.LOSS_CONFIG["loss_combination"]}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return plot_path


def create_prediction_examples(model, test_loader, config, output_dir, device):
    """åˆ›å»ºé¢„æµ‹ç¤ºä¾‹å›¾"""
    model.eval()
    
    with torch.no_grad():
        for batch_data in test_loader:
            if len(batch_data) == 3:
                features, labels, tid_weights = batch_data
                features = features.to(device)
                labels = labels.to(device)
            else:
                features, labels = batch_data
                features = features.to(device)
                labels = labels.to(device)
            
            predictions = model(features)
            break
    
    n_examples = min(4, predictions.size(0))
    
    fig, axes = plt.subplots(n_examples, 3, figsize=(15, 5*n_examples))
    if n_examples == 1:
        axes = axes.reshape(1, -1)
    
    for i in range(n_examples):
        pred = predictions[i, 0].cpu().numpy()
        true = labels[i, 0].cpu().numpy()
        error = pred - true
        
        im1 = axes[i, 0].imshow(true, cmap='RdBu_r', aspect='auto')
        axes[i, 0].set_title(f'çœŸå€¼ {i+1}')
        plt.colorbar(im1, ax=axes[i, 0])
        
        im2 = axes[i, 1].imshow(pred, cmap='RdBu_r', aspect='auto')
        axes[i, 1].set_title(f'é¢„æµ‹ {i+1}')
        plt.colorbar(im2, ax=axes[i, 1])
        
        im3 = axes[i, 2].imshow(error, cmap='RdBu_r', aspect='auto')
        axes[i, 2].set_title(f'è¯¯å·® {i+1}')
        plt.colorbar(im3, ax=axes[i, 2])
    
    plt.suptitle(f'é¢„æµ‹ç¤ºä¾‹ - {config.LOSS_CONFIG["loss_combination"]}', fontsize=14)
    plt.tight_layout()
    
    examples_path = os.path.join(output_dir, f'prediction_examples_{config.LOSS_CONFIG["loss_combination"]}.png')
    plt.savefig(examples_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return examples_path


def create_scatter_plot(model, test_loader, config, output_dir, device):
    """åˆ›å»ºæ•£ç‚¹å›¾"""
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_data in test_loader:
            if len(batch_data) == 3:
                features, labels, _ = batch_data
                features = features.to(device)
                labels = labels.to(device)
            else:
                features, labels = batch_data
                features = features.to(device)
                labels = labels.to(device)
            
            predictions = model(features)
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(labels.cpu().numpy())
    
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    pred_flat = predictions.flatten()
    target_flat = targets.flatten()
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    mse = np.mean((pred_flat - target_flat) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(pred_flat - target_flat))
    correlation = np.corrcoef(pred_flat, target_flat)[0, 1]
    
    # åˆ›å»ºæ•£ç‚¹å›¾
    fig, ax = plt.subplots(figsize=(10, 10))
    
    n_samples = min(50000, len(pred_flat))
    indices = np.random.choice(len(pred_flat), n_samples, replace=False)
    pred_sample = pred_flat[indices]
    target_sample = target_flat[indices]
    
    ax.scatter(target_sample, pred_sample, alpha=0.3, s=1, c='blue')
    
    min_val = min(target_sample.min(), pred_sample.min())
    max_val = max(target_sample.max(), pred_sample.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹')
    
    ax.set_xlabel('çœŸå€¼')
    ax.set_ylabel('é¢„æµ‹å€¼')
    ax.set_title(f'é¢„æµ‹ vs çœŸå€¼æ•£ç‚¹å›¾\n'
                f'RMSE: {rmse:.4f}, MAE: {mae:.4f}, ç›¸å…³ç³»æ•°: {correlation:.4f}')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xlim(min_val, max_val)
    ax.set_ylim(min_val, max_val)
    ax.set_aspect('equal')
    
    plt.tight_layout()
    
    scatter_path = os.path.join(output_dir, f'scatter_plot_{config.LOSS_CONFIG["loss_combination"]}.png')
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return scatter_path, {'rmse': float(rmse), 'mae': float(mae), 'correlation': float(correlation)}


# é…ç½®ç±»å®šä¹‰ï¼ˆä¿æŒä¸ä¹‹å‰ç›¸åŒï¼‰
class BaselineConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-baseline_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'baseline',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.0,
            'spectral_loss_weight': 0.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': False,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class WeightedMSEConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-weighted_mse_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'weighted_mse',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.0,
            'spectral_loss_weight': 0.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class MSEGradientConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-mse_grad_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'weighted_mse_grad',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.3,
            'spectral_loss_weight': 0.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class MSESpectralConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-mse_fft_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'weighted_mse_fft',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.0,
            'spectral_loss_weight': 1.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class AllCombinedConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-all_combined_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'all',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.3,
            'spectral_loss_weight': 1.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }


def get_config(config_name):
    """æ ¹æ®é…ç½®åç§°è·å–é…ç½®å¯¹è±¡"""
    config_map = {
        'baseline': BaselineConfig,
        'weighted_mse': WeightedMSEConfig,
        'mse_grad': MSEGradientConfig,
        'mse_fft': MSESpectralConfig,
        'all_combined': AllCombinedConfig,
    }
    
    if config_name not in config_map:
        available_configs = list(config_map.keys())
        raise ValueError(f"æœªçŸ¥é…ç½®: {config_name}. å¯ç”¨é…ç½®: {available_configs}")
    
    return config_map[config_name]()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ - é›†æˆWandB')
    parser.add_argument('--config', type=str, required=True,
                        choices=['baseline', 'weighted_mse', 'mse_grad', 'mse_fft', 'all_combined'],
                        help='é€‰æ‹©è®­ç»ƒé…ç½®')
    parser.add_argument('--epochs', type=int, default=None, help='è¦†ç›–é»˜è®¤çš„è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=None, help='è¦†ç›–é»˜è®¤çš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=None, help='è¦†ç›–é»˜è®¤çš„å­¦ä¹ ç‡')
    parser.add_argument('--use_wandb', action='store_true', help='ä½¿ç”¨WandBè¿›è¡Œå®éªŒè·Ÿè¸ª')
    parser.add_argument('--wandb_project', type=str, default='seafloor-topography', help='WandBé¡¹ç›®åç§°')
    parser.add_argument('--quiet', action='store_true', help='å®‰é™æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.quiet)
    
    # è·å–é…ç½®
    config = get_config(args.config)
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.epochs is not None:
        config.NUM_EPOCHS = args.epochs
    if args.batch_size is not None:
        config.BATCH_SIZE = args.batch_size
    if args.learning_rate is not None:
        config.LEARNING_RATE = args.learning_rate
    
    if not args.quiet:
        logger.info(f"ä½¿ç”¨é…ç½®: {args.config}")
        logger.info(f"æŸå¤±å‡½æ•°ç»„åˆ: {config.LOSS_CONFIG['loss_combination']}")
        logger.info(f"è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
        logger.info(f"ä½¿ç”¨WandB: {args.use_wandb and WANDB_AVAILABLE}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if not args.quiet:
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.RANDOM_SEED)
    np.random.seed(config.RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.RANDOM_SEED)
        torch.cuda.manual_seed_all(config.RANDOM_SEED)

    # æ•°æ®å‡†å¤‡
    data_prep = enhanced_training_multi_loss.EnhancedDataPreparation(config)

    try:
        # åŠ è½½æ•°æ®
        features_ds, labels_da, tid_weights_da = data_prep.load_data()
        patches = data_prep.extract_patches(features_ds, labels_da, tid_weights_da)

        if len(patches) == 0:
            logger.error("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„æ•°æ®å—ï¼")
            return

        # åˆ’åˆ†æ•°æ®é›†
        train_patches, val_patches, test_patches = data_prep.split_patches(patches)

        # åˆ›å»ºæ•°æ®é›†
        train_dataset = enhanced_training_multi_loss.EnhancedBathymetryDataset(train_patches, config, is_training=True)
        val_dataset = enhanced_training_multi_loss.EnhancedBathymetryDataset(val_patches, config, is_training=False)
        test_dataset = enhanced_training_multi_loss.EnhancedBathymetryDataset(test_patches, config, is_training=False)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, 
                                num_workers=config.NUM_WORKERS, pin_memory=torch.cuda.is_available(), drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, 
                              num_workers=config.NUM_WORKERS, pin_memory=torch.cuda.is_available())
        test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, 
                                num_workers=config.NUM_WORKERS, pin_memory=torch.cuda.is_available())

        # åˆ›å»ºæ¨¡å‹
        model = enhanced_training_multi_loss.OptimizedUNet(
            in_channels=config.IN_CHANNELS,
            out_channels=config.OUT_CHANNELS,
            init_features=config.INIT_FEATURES
        )

        total_params = sum(p.numel() for p in model.parameters())
        if not args.quiet:
            logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = WandBTrainer(model, config, use_wandb=args.use_wandb, project_name=args.wandb_project)
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train(train_loader, val_loader, quiet=args.quiet)

        # åˆ›å»ºå¯è§†åŒ–
        if not args.quiet:
            logger.info("åˆ›å»ºå¯è§†åŒ–...")
        
        plot_path = create_training_visualization(trainer, config, config.OUTPUT_DIR)
        examples_path = create_prediction_examples(model, test_loader, config, config.OUTPUT_DIR, device)
        scatter_path, test_metrics = create_scatter_plot(model, test_loader, config, config.OUTPUT_DIR, device)

        # ä¿å­˜å®éªŒæ‘˜è¦
        experiment_summary = {
            'experiment_name': f'enhanced_multi_loss_{config.LOSS_CONFIG["loss_combination"]}',
            'loss_config': config.LOSS_CONFIG,
            'model_config': {
                'in_channels': config.IN_CHANNELS,
                'out_channels': config.OUT_CHANNELS,
                'init_features': config.INIT_FEATURES
            },
            'training_config': {
                'batch_size': config.BATCH_SIZE,
                'learning_rate': config.LEARNING_RATE,
                'num_epochs': len(trainer.history['train_loss_total']),
                'early_stopping_patience': config.EARLY_STOPPING_PATIENCE
            },
            'best_val_loss': float(trainer.best_val_loss),  # ç¡®ä¿è½¬æ¢ä¸ºPython float
            'total_params': int(total_params),  # ç¡®ä¿è½¬æ¢ä¸ºPython int
            'trainable_params': int(total_params),  # ç¡®ä¿è½¬æ¢ä¸ºPython int
            'test_metrics': {
                'rmse': float(test_metrics['rmse']),  # è½¬æ¢ä¸ºPython float
                'mae': float(test_metrics['mae']),  # è½¬æ¢ä¸ºPython float
                'correlation': float(test_metrics['correlation'])  # è½¬æ¢ä¸ºPython float
            },
            'use_wandb': args.use_wandb and WANDB_AVAILABLE
        }

        summary_path = os.path.join(config.OUTPUT_DIR, 
                                f'experiment_summary_{config.LOSS_CONFIG["loss_combination"]}_lt135km.json')
        with open(summary_path, 'w') as f:
            json.dump(experiment_summary, f, indent=2)

        # è¾“å‡ºç»“æœæ‘˜è¦
        if not args.quiet:
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æŸå¤±ç»„åˆ: {config.LOSS_CONFIG['loss_combination']}")
            logger.info(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {trainer.best_val_loss:.6f}")
            logger.info(f"ğŸ“ˆ æµ‹è¯•RMSE: {test_metrics['rmse']:.6f}")
            logger.info(f"ğŸ“ˆ æµ‹è¯•ç›¸å…³ç³»æ•°: {test_metrics['correlation']:.6f}")
            logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config.OUTPUT_DIR}")
        else:
            print(f"âœ… {args.config}: Val={trainer.best_val_loss:.4f}, RMSE={test_metrics['rmse']:.4f}, Corr={test_metrics['correlation']:.4f}")

    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()