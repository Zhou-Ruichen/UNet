#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤§åŒºåŸŸæµ·åº•åœ°å½¢é¢„æµ‹å™¨
åŸºäº04-UNet_SWOTé¡¹ç›®çš„æˆåŠŸç»éªŒï¼Œå®ç°å¤šæŸå¤±å‡½æ•°æ¨¡å‹çš„å¤§åŒºåŸŸé¢„æµ‹
"""

import os
import numpy as np
import xarray as xr
import torch
import torch.nn as nn
from tqdm import tqdm
import logging
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from datetime import datetime

# å¯¼å…¥ç°æœ‰æ¨¡å—
from enhanced_training_multi_loss import OptimizedUNet, EnhancedTrainingConfig
from training_variants import BaselineConfig, WeightedMSEConfig, MSEGradientConfig, MSESpectralConfig, AllCombinedConfig

logger = logging.getLogger('large_area_predictor')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class LargeAreaPredictionConfig:
    """å¤§åŒºåŸŸé¢„æµ‹é…ç½®"""
    # æ•°æ®è·¯å¾„
    DATA_DIR = "/mnt/data5/06-Projects/05-unet/output/1-refined_seafloor_data_preparation"
    FEATURES_PATH = os.path.join(DATA_DIR, "Features_SWOT_LT135km_Standardized.nc")
    LONGWAVE_PATH = os.path.join(DATA_DIR, "Bathy_True_Longwave.nc")
    TRUTH_PATH = "/mnt/data5/00-Data/nc/GEBCO_2024.nc"
    
    # é¢„æµ‹å‚æ•° - å€Ÿé‰´æˆåŠŸç»éªŒ
    PATCH_SIZE = 128
    STRIDE = PATCH_SIZE // 16  # 93.8%é‡å ç‡
    BATCH_SIZE = 16
    
    # æ¨¡å‹å‚æ•°
    IN_CHANNELS = 4
    OUT_CHANNELS = 1
    INIT_FEATURES = 48
    
    # é¢„æµ‹åŒºåŸŸ
    PREDICTION_REGION = {
        "lat_range": slice(0, 50),  # æ‰©å¤§é¢„æµ‹åŒºåŸŸ
        "lon_range": slice(100, 130),
        "name": "å—æµ·æ‰©å±•åŒºåŸŸ"
    }
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/large_area_predictions"

class LargeAreaPredictor:
    """å¤§åŒºåŸŸé¢„æµ‹å™¨"""
    
    def __init__(self, config: LargeAreaPredictionConfig):
        self.config = config
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)
        
        # æ¨¡å‹é…ç½®æ˜ å°„ - æ ¹æ®å®é™…ç›®å½•ç»“æ„æ›´æ–°
        self.model_configs = {
            'baseline': ('2-baseline_outputs_lt135km', 'åŸºçº¿MSE'),
            'weighted_mse': ('2-weighted_mse_outputs_lt135km', 'TIDæƒé‡MSE'),
            'mse_grad': ('2-mse_grad_outputs_lt135km', 'MSE+æ¢¯åº¦æŸå¤±'),
            'mse_fft': ('2-mse_fft_outputs_lt135km', 'MSE+é¢‘è°±æŸå¤±'),
            'all_combined': ('2-all_combined_outputs_lt135km', 'å…¨æŸå¤±ç»„åˆ'),
            'enhanced_multi_loss': ('2-enhanced_multi_loss_outputs_lt135km', 'å¢å¼ºå¤šæŸå¤±')
        }
    
    def create_gaussian_weight_kernel(self, patch_size: int) -> np.ndarray:
        """åˆ›å»ºé«˜æ–¯æƒé‡æ ¸ - å€Ÿé‰´æˆåŠŸæ–¹æ³•"""
        y, x = np.ogrid[:patch_size, :patch_size]
        center = patch_size / 2.0
        sigma = center * 0.5
        
        if sigma <= 0:
            sigma = 1.0
        
        weight_kernel = np.exp(-0.5 * ((x - (center - 0.5))**2 + (y - (center - 0.5))**2) / sigma**2)
        return weight_kernel.astype(np.float32)
    
    def load_model(self, model_path: str) -> Optional[nn.Module]:
        """åŠ è½½æ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=device)
            
            model = OptimizedUNet(
                in_channels=self.config.IN_CHANNELS,
                out_channels=self.config.OUT_CHANNELS,
                init_features=self.config.INIT_FEATURES
            )
            
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            model.to(device)
            model.eval()
            logger.info(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return model
        
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ {model_path}: {e}")
            return None
    
    def load_data(self) -> Tuple[xr.Dataset, xr.DataArray]:
        """åŠ è½½é¢„æµ‹æ•°æ®"""
        logger.info("åŠ è½½æ•°æ®...")
        
        # åŠ è½½ç‰¹å¾
        features = xr.open_dataset(self.config.FEATURES_PATH)
        
        # åŠ è½½é•¿æ³¢èƒŒæ™¯
        longwave = xr.open_dataarray(self.config.LONGWAVE_PATH)
        
        # åº”ç”¨åŒºåŸŸé€‰æ‹©
        if self.config.PREDICTION_REGION:
            region = self.config.PREDICTION_REGION
            features = features.sel(
                lat=region['lat_range'],
                lon=region['lon_range']
            )
            longwave = longwave.sel(
                lat=region['lat_range'],
                lon=region['lon_range']
            )
        
        logger.info(f"æ•°æ®å½¢çŠ¶: features={features.sizes}, longwave={longwave.shape}")
        return features, longwave
    
    def predict_large_area(self, model: nn.Module, features: xr.Dataset) -> np.ndarray:
        """å¤§åŒºåŸŸé¢„æµ‹ - å€Ÿé‰´æˆåŠŸçš„æ‹¼æ¥ç­–ç•¥"""
        logger.info("å¼€å§‹å¤§åŒºåŸŸé¢„æµ‹...")
        
        lat_size = len(features.lat)
        lon_size = len(features.lon)
        
        # åˆå§‹åŒ–è¾“å‡º
        prediction = np.zeros((lat_size, lon_size), dtype=np.float32)
        count_map = np.zeros((lat_size, lon_size), dtype=np.float32)
        
        # åˆ›å»ºé«˜æ–¯æƒé‡æ ¸
        weight_kernel = self.create_gaussian_weight_kernel(self.config.PATCH_SIZE)
        
        # æ”¶é›†patches
        patches = []
        positions = []
        
        for i in range(0, lat_size - self.config.PATCH_SIZE + 1, self.config.STRIDE):
            for j in range(0, lon_size - self.config.PATCH_SIZE + 1, self.config.STRIDE):
                lat_start, lon_start = i, j
                lat_end = lat_start + self.config.PATCH_SIZE
                lon_end = lon_start + self.config.PATCH_SIZE
                
                # æå–ç‰¹å¾patch
                feature_patches = []
                all_valid = True
                
                for var in ['GA', 'DOV_EW', 'DOV_NS', 'VGG']:
                    if var in features:
                        patch = features[var].isel(
                            lat=slice(lat_start, lat_end),
                            lon=slice(lon_start, lon_end)
                        ).values
                        
                        if np.all(np.isnan(patch)):
                            all_valid = False
                            break
                        
                        feature_patches.append(patch)
                    else:
                        all_valid = False
                        break
                
                if all_valid and len(feature_patches) == 4:
                    feature_array = np.stack(feature_patches, axis=0)
                    feature_array = np.nan_to_num(feature_array, 0)
                    
                    patches.append(feature_array)
                    positions.append((lat_start, lon_start, lat_end, lon_end))
        
        logger.info(f"æå–äº† {len(patches)} ä¸ªæœ‰æ•ˆpatch")
        
        # æ‰¹é‡é¢„æµ‹
        with torch.no_grad():
            for i in tqdm(range(0, len(patches), self.config.BATCH_SIZE), desc="é¢„æµ‹è¿›åº¦"):
                batch_patches = patches[i:i + self.config.BATCH_SIZE]
                batch_positions = positions[i:i + self.config.BATCH_SIZE]
                
                # è½¬æ¢ä¸ºå¼ é‡
                batch_tensor = torch.from_numpy(
                    np.array(batch_patches)
                ).float().to(device)
                
                # é¢„æµ‹
                batch_pred = model(batch_tensor)
                batch_pred = batch_pred.cpu().numpy()
                
                # æƒé‡ç´¯ç§¯
                for j, (lat_start, lon_start, lat_end, lon_end) in enumerate(batch_positions):
                    pred_patch = batch_pred[j, 0]
                    
                    # å¤„ç†å¼‚å¸¸å€¼
                    pred_patch = np.nan_to_num(pred_patch, 0)
                    
                    # æƒé‡ç´¯ç§¯
                    weighted_output = pred_patch * weight_kernel
                    
                    prediction[lat_start:lat_end, lon_start:lon_end] += weighted_output
                    count_map[lat_start:lat_end, lon_start:lon_end] += weight_kernel
        
        # å®‰å…¨å½’ä¸€åŒ–
        count_map_safe = np.where(count_map > 1e-9, count_map, 1.0)
        prediction_averaged = np.divide(prediction, count_map_safe)
        
        logger.info("âœ“ å¤§åŒºåŸŸé¢„æµ‹å®Œæˆ")
        return prediction_averaged
    
    def predict_all_models(self):
            """ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œå¤§åŒºåŸŸé¢„æµ‹"""
            logger.info("=== å¼€å§‹å¤šæ¨¡å‹å¤§åŒºåŸŸé¢„æµ‹ ===")
            
            features, longwave = self.load_data()
            results = {}
            
            for model_name, (output_dir, description) in self.model_configs.items():
                logger.info(f"\nå¤„ç†æ¨¡å‹: {description}")
                
                # --- MODIFICATION START ---
                # Adjust model_file_key_part to match the varying parts of the actual filenames
                model_file_key_part = model_name 
                if model_name == 'mse_grad':
                    model_file_key_part = 'weighted_mse_grad'
                elif model_name == 'mse_fft':
                    model_file_key_part = 'weighted_mse_fft'
                elif model_name == 'all_combined':
                    model_file_key_part = 'all'
                # For 'enhanced_multi_loss', if its filename was, for example,
                # 'best_model_enhanced_multi_loss_lt135km.pth' (without repeating 'enhanced_multi_loss'),
                # a specific rule would be needed here.
                # If model_name == 'enhanced_multi_loss':
                # model_file_key_part = '' # Or some other specific identifier

                # Construct the most likely filename based on observed patterns
                # e.g., best_model_enhanced_multi_loss_baseline_lt135km.pth
                specific_expected_filename = f'best_model_enhanced_multi_loss_{model_file_key_part}_lt135km.pth'
                
                # Special case for 'enhanced_multi_loss' if its naming is simpler and known
                # This is an example if the 'enhanced_multi_loss' model filename was simpler,
                # and assuming 'enhanced_multi_loss' is the unique identifier part.
                # if model_name == 'enhanced_multi_loss' and model_file_key_part == 'enhanced_multi_loss':
                #     # Example: if filename was just 'best_model_enhanced_multi_loss_lt135km.pth'
                #     specific_expected_filename = 'best_model_enhanced_multi_loss_lt135km.pth'


                possible_model_files = [
                    specific_expected_filename,  # Try the most accurately constructed name first
                    # Original fallbacks, in case some models use simpler naming
                    'best_model.pth',
                    'model_best.pth', 
                    'final_model.pth',
                    f'best_model_{model_name}.pth' # Original generic pattern based on model_name
                ]
                # --- MODIFICATION END ---
                
                model_path = None
                # Construct the full path to the directory where models for the current configuration are stored
                model_directory_path = f"/mnt/data5/06-Projects/05-unet/output/{output_dir}"

                for filename_to_check in possible_model_files:
                    candidate_path = os.path.join(model_directory_path, filename_to_check)
                    if os.path.exists(candidate_path):
                        model_path = candidate_path
                        logger.info(f"Found model file: {model_path}") # Log successful finding
                        break
                
                if model_path is None:
                    logger.warning(f"åœ¨ {output_dir} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶. Tried patterns: {possible_model_files}")
                    # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ä»¥ä¾¿è°ƒè¯• (already present and helpful)
                    try:
                        files = os.listdir(model_directory_path)
                        logger.info(f"ç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
                    except FileNotFoundError:
                        logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {model_directory_path}")
                    except Exception as e:
                        logger.warning(f"æ— æ³•è®¿é—®ç›®å½•: {model_directory_path}. Error: {e}")
                    continue
                
                # åŠ è½½æ¨¡å‹
                model = self.load_model(model_path)
                if model is None:
                    continue
                
                # é¢„æµ‹
                prediction = self.predict_large_area(model, features)
                
                # ä¿å­˜ç»“æœ (remainder of the loop is likely fine)
                result_da = xr.DataArray(
                    prediction,
                    coords=features.coords,
                    dims=['lat', 'lon'],
                    name=f'prediction_{model_name}'
                )
                
                output_path = os.path.join(
                    self.config.OUTPUT_DIR, 
                    f'large_area_prediction_{model_name}.nc'
                )
                result_da.to_netcdf(output_path)
                
                results[model_name] = {
                    'prediction': result_da,
                    'description': description,
                    'output_path': output_path
                }
                
                logger.info(f"âœ“ {description} é¢„æµ‹å®Œæˆï¼Œä¿å­˜åˆ°: {output_path}")
            
            # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
            if results:
                self.create_comparison_visualization(results, features)
                logger.info(f"\nğŸ‰ éƒ¨åˆ†æˆ–æ‰€æœ‰æ¨¡å‹å¤§åŒºåŸŸé¢„æµ‹å®Œæˆï¼") # Adjusted message
                logger.info(f"ç»“æœä¿å­˜åœ¨: {self.config.OUTPUT_DIR}")
            else:
                logger.error("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½å¹¶å¤„ç†ä»»ä½•æ¨¡å‹") # Adjusted message
            
            return results
    
    def create_comparison_visualization(self, results: Dict, features: xr.Dataset):
        """åˆ›å»ºå¤šæ¨¡å‹å¯¹æ¯”å¯è§†åŒ–"""
        logger.info("åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–...")
        
        n_models = len(results)
        if n_models == 0:
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        # æ˜¾ç¤ºåŸå§‹é‡åŠ›å¼‚å¸¸
        if 'GA' in features:
            im0 = axes[0].pcolormesh(
                features.lon, features.lat, features['GA'].values,
                cmap='RdBu_r', shading='auto'
            )
            axes[0].set_title('SWOTé‡åŠ›å¼‚å¸¸')
            plt.colorbar(im0, ax=axes[0])
        
        # æ˜¾ç¤ºå„æ¨¡å‹é¢„æµ‹ç»“æœ
        for i, (model_name, result_info) in enumerate(results.items()):
            if i + 1 >= len(axes):
                break
            
            prediction = result_info['prediction']
            im = axes[i + 1].pcolormesh(
                prediction.lon, prediction.lat, prediction.values,
                cmap='terrain', shading='auto'
            )
            axes[i + 1].set_title(f"{result_info['description']}")
            plt.colorbar(im, ax=axes[i + 1])
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(results) + 1, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_path = os.path.join(self.config.OUTPUT_DIR, 'large_area_comparison.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"âœ“ å¯¹æ¯”å›¾ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('large_area_prediction.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    config = LargeAreaPredictionConfig()
    predictor = LargeAreaPredictor(config)
    
    try:
        results = predictor.predict_all_models()
        logger.info("âœ… å¤§åŒºåŸŸé¢„æµ‹ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        return 0
    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())