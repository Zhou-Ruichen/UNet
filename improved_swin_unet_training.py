#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„Swin U-Netè®­ç»ƒ - æ”¯æŒæŸå¤±å‡½æ•°é…ç½®é€‰æ‹©

æ–°å¢åŠŸèƒ½ï¼š
1. æ”¯æŒå¤šç§é¢„è®¾æŸå¤±é…ç½®ï¼ˆbaselineã€TIDåŠ æƒã€æ¢¯åº¦å¢å¼ºï¼‰
2. ç§»é™¤FFTæŸå¤±é€‰é¡¹ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼‰
3. ä¼˜åŒ–è®­ç»ƒå‚æ•°å’Œç›‘æ§
4. ç®€åŒ–çš„é…ç½®é€‰æ‹©ç•Œé¢

ä½¿ç”¨æ–¹æ³•:
python improved_swin_unet_training.py --loss_config baseline
python improved_swin_unet_training.py --loss_config tid_weighted
python improved_swin_unet_training.py --loss_config tid_gradient
python improved_swin_unet_training.py --loss_config enhanced_gradient

ä½œè€…: å‘¨ç‘å®¸
æ—¥æœŸ: 2025-05-26
ç‰ˆæœ¬: v1.2 (æŸå¤±å‡½æ•°é…ç½®åŒ–)
"""

import argparse
import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import json
import logging
from datetime import datetime
from tqdm import tqdm

# å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from enhanced_training_multi_loss import (
    EnhancedTrainingConfig, EnhancedDataPreparation,
    EnhancedBathymetryDataset, MultiLossFunction,
    device, setup_chinese_font
)

# å¯¼å…¥Swin U-Netæ¨¡å‹ï¼ˆä»ç”¨æˆ·æä¾›çš„ä»£ç ï¼‰
from swin_unet_training import (
    SwinUNet, SwinTransformerBlock, BasicLayer, PatchMerging,
    WindowAttention, DecoderBlock, DropPath, drop_path,
    window_partition, window_reverse
)

setup_chinese_font()
logger = logging.getLogger('improved_swin_unet')


# Swin U-NetæŸå¤±å‡½æ•°é…ç½®é¢„è®¾
SWIN_LOSS_CONFIGS = {
    'baseline': {
        'name': 'Baseline',
        'loss_combination': 'baseline',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.0,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': False,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.0,
        'description': 'æ ‡å‡†MSEæŸå¤±ï¼Œä¸ä½¿ç”¨æƒé‡å’Œé¢å¤–æŸå¤±'
    },
    
    'tid_weighted': {
        'name': 'TIDåŠ æƒ',
        'loss_combination': 'weighted_mse',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.0,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.0,
        'description': 'ä½¿ç”¨TIDæƒé‡çš„MSEæŸå¤±ï¼Œé‡ç‚¹å…³æ³¨é«˜è´¨é‡åŒºåŸŸ'
    },
    
    'tid_gradient': {
        'name': 'TIDæƒé‡+æ¢¯åº¦',
        'loss_combination': 'weighted_mse_grad',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.3,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.0,
        'description': 'TIDæƒé‡MSE + æ¢¯åº¦æŸå¤±ï¼Œå¢å¼ºç»†èŠ‚å’Œè¾¹ç¼˜'
    },
    
    'enhanced_gradient': {
        'name': 'å¢å¼ºæ¢¯åº¦',
        'loss_combination': 'weighted_mse_grad',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.5,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.2,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.5,
        'description': 'å¢å¼ºçš„æ¢¯åº¦æŸå¤±æƒé‡ï¼Œæœ€å¤§åŒ–ç»†èŠ‚æ¢å¤èƒ½åŠ›'
    }
}


class ImprovedSwinUNetConfig(EnhancedTrainingConfig):
    """æ”¹è¿›çš„Swin U-Neté…ç½® - æ”¯æŒæŸå¤±å‡½æ•°é€‰æ‹©"""
    
    def __init__(self, loss_config_name='tid_gradient'):
        super().__init__()
        
        # éªŒè¯æŸå¤±é…ç½®
        if loss_config_name not in SWIN_LOSS_CONFIGS:
            available = list(SWIN_LOSS_CONFIGS.keys())
            raise ValueError(f"æœªçŸ¥çš„æŸå¤±é…ç½®: {loss_config_name}. å¯ç”¨é…ç½®: {available}")
        
        # åŸºç¡€é…ç½®
        self.OUTPUT_DIR = f"/mnt/data5/06-Projects/05-unet/output/3-swin_{loss_config_name}_outputs_lt135km"
        
        # åº”ç”¨é€‰æ‹©çš„æŸå¤±é…ç½®
        loss_config = SWIN_LOSS_CONFIGS[loss_config_name]
        self.LOSS_CONFIG = loss_config.copy()
        
        # Swin Transformerç‰¹å®šå‚æ•°
        self.SWIN_CONFIG = {
            'patch_size': 4,
            'in_chans': 4,
            'embed_dim': 96,
            'depths': [2, 2, 6, 2],
            'num_heads': [3, 6, 12, 24],
            'window_size': 8,
            'mlp_ratio': 4.0,
            'drop_rate': 0.0,
            'attn_drop_rate': 0.0,
            'drop_path_rate': 0.1,
            'use_checkpoint': False,
            'decoder_channels': [512, 256, 128, 64],
            'skip_connection_type': 'concat',
            'final_upsample': 'expand_first',
        }
        
        # è®­ç»ƒå‚æ•°ä¼˜åŒ–
        self.BATCH_SIZE = 6  # é€‚åˆSwin U-Netçš„æ‰¹æ¬¡å¤§å°
        self.LEARNING_RATE = 1e-4
        self.NUM_EPOCHS = 150
        self.WEIGHT_DECAY = 0.05
        self.EARLY_STOPPING_PATIENCE = 20
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        self.loss_config_name = loss_config_name
        self.loss_config_description = loss_config['description']
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸ç”¨äºä¿å­˜"""
        return {
            key: value for key, value in self.__dict__.items()
            if not key.startswith('__') and not callable(value)
        }


class ImprovedSwinUNetTrainer:
    """æ”¹è¿›çš„Swin U-Netè®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, config: ImprovedSwinUNetConfig):
        self.model = model.to(device)
        self.config = config
        
        # ä½¿ç”¨ç°æœ‰çš„å¤šé‡æŸå¤±å‡½æ•°
        self.criterion = MultiLossFunction(config.LOSS_CONFIG)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=config.LEARNING_RATE, 
            weight_decay=config.WEIGHT_DECAY
        )
        
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=config.NUM_EPOCHS, 
            eta_min=config.LEARNING_RATE * 0.01
        )
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss_total': [], 
            'train_loss_pixel': [], 
            'train_loss_gradient': [], 
            'train_loss_spectral': [],
            'val_loss_total': [], 
            'val_loss_pixel': [], 
            'val_loss_gradient': [], 
            'val_loss_spectral': [],
            'val_correlation': [], 
            'learning_rates': []
        }
        
        self.best_val_loss = float('inf')
        self.best_val_correlation = -1.0
        self.early_stopping_counter = 0
    
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = {
            'total': 0.0, 
            'pixel': 0.0, 
            'gradient': 0.0, 
            'spectral': 0.0
        }
        
        progress_bar = tqdm(train_loader, desc=f'è®­ç»ƒ Epoch {epoch}', leave=False)
        
        for batch_idx, batch_data in enumerate(progress_bar):
            # è·å–æ•°æ®
            features = batch_data[0].to(device)
            labels = batch_data[1].to(device)
            tid_weights = batch_data[2].to(device) if len(batch_data) == 3 and self.config.LOSS_CONFIG['use_tid_weights'] else None
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(features)
            
            # ç¡®ä¿è¾“å‡ºå°ºå¯¸åŒ¹é…
            if outputs.shape[-2:] != labels.shape[-2:]:
                outputs = F.interpolate(outputs, size=labels.shape[-2:], 
                                      mode='bilinear', align_corners=False)
            
            # è®¡ç®—æŸå¤±
            loss, loss_components = self.criterion(outputs, labels, tid_weights)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            for key in epoch_losses:
                if key in loss_components:
                    value = loss_components[key]
                    if isinstance(value, torch.Tensor):
                        epoch_losses[key] += value.item()
                    elif isinstance(value, (int, float)):
                        epoch_losses[key] += value
            
            # æ›´æ–°è¿›åº¦æ¡
            postfix_dict = {}
            for k, v in loss_components.items():
                if k in ['total', 'pixel', 'gradient']:
                    if isinstance(v, torch.Tensor):
                        postfix_dict[k] = f'{v.item():.4f}'
                    elif isinstance(v, (int, float)):
                        postfix_dict[k] = f'{v:.4f}'
            progress_bar.set_postfix(postfix_dict)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        n_batches = len(train_loader)
        return {key: val / n_batches if n_batches > 0 else 0.0 for key, val in epoch_losses.items()}
    
    def validate(self, val_loader: DataLoader) -> dict:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        epoch_losses = {
            'total': 0.0, 
            'pixel': 0.0, 
            'gradient': 0.0, 
            'spectral': 0.0
        }
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_data in tqdm(val_loader, desc='éªŒè¯ä¸­', leave=False):
                features = batch_data[0].to(device)
                labels = batch_data[1].to(device)
                tid_weights = batch_data[2].to(device) if len(batch_data) == 3 and self.config.LOSS_CONFIG['use_tid_weights'] else None
                
                # é¢„æµ‹
                outputs = self.model(features)
                
                # ç¡®ä¿å°ºå¯¸åŒ¹é…
                if outputs.shape[-2:] != labels.shape[-2:]:
                    outputs = F.interpolate(outputs, size=labels.shape[-2:], 
                                          mode='bilinear', align_corners=False)
                
                # è®¡ç®—æŸå¤±
                loss, loss_components = self.criterion(outputs, labels, tid_weights)
                
                # è®°å½•æŸå¤±
                for key in epoch_losses:
                    if key in loss_components:
                        value = loss_components[key]
                        if isinstance(value, torch.Tensor):
                            epoch_losses[key] += value.item()
                        elif isinstance(value, (int, float)):
                            epoch_losses[key] += value
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions.append(outputs.cpu().numpy())
                all_targets.append(labels.cpu().numpy())
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = 0.0
        if all_predictions and all_targets:
            predictions = np.concatenate(all_predictions, axis=0)
            targets = np.concatenate(all_targets, axis=0)
            
            pred_flat = predictions.flatten()
            target_flat = targets.flatten()
            
            if len(pred_flat) > 1 and len(target_flat) > 1:
                correlation = np.corrcoef(pred_flat, target_flat)[0, 1]
                if np.isnan(correlation):
                    correlation = 0.0
        
        # è®¡ç®—å¹³å‡æŸå¤±
        n_batches = len(val_loader)
        results = {key: val / n_batches if n_batches > 0 else 0.0 for key, val in epoch_losses.items()}
        results['correlation'] = correlation
        
        return results
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info(f"å¼€å§‹æ”¹è¿›çš„Swin U-Netè®­ç»ƒ")
        logger.info(f"æŸå¤±é…ç½®: {self.config.loss_config_name} - {self.config.loss_config_description}")
        
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        
        for epoch in range(1, self.config.NUM_EPOCHS + 1):
            logger.info(f"\nEpoch {epoch}/{self.config.NUM_EPOCHS}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_metrics = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # æ›´æ–°å†å²è®°å½•
            self.history['train_loss_total'].append(train_metrics.get('total', 0.0))
            self.history['train_loss_pixel'].append(train_metrics.get('pixel', 0.0))
            self.history['train_loss_gradient'].append(train_metrics.get('gradient', 0.0))
            self.history['train_loss_spectral'].append(train_metrics.get('spectral', 0.0))
            
            self.history['val_loss_total'].append(val_metrics.get('total', 0.0))
            self.history['val_loss_pixel'].append(val_metrics.get('pixel', 0.0))
            self.history['val_loss_gradient'].append(val_metrics.get('gradient', 0.0))
            self.history['val_loss_spectral'].append(val_metrics.get('spectral', 0.0))
            
            self.history['val_correlation'].append(val_metrics.get('correlation', 0.0))
            self.history['learning_rates'].append(current_lr)
            
            # æ‰“å°æŒ‡æ ‡
            train_log = f"æ€»æŸå¤±: {train_metrics.get('total', 0):.6f}"
            if train_metrics.get('pixel', 0) > 0:
                train_log += f", åƒç´ : {train_metrics.get('pixel', 0):.6f}"
            if train_metrics.get('gradient', 0) > 0:
                train_log += f", æ¢¯åº¦: {train_metrics.get('gradient', 0):.6f}"
            
            val_log = f"æ€»æŸå¤±: {val_metrics.get('total', 0):.6f}, ç›¸å…³ç³»æ•°: {val_metrics.get('correlation', 0):.6f}"
            
            logger.info(f"è®­ç»ƒ - {train_log}")
            logger.info(f"éªŒè¯ - {val_log}")
            logger.info(f"å­¦ä¹ ç‡: {current_lr:.6e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
            val_loss = val_metrics.get('total', float('inf'))
            val_correlation = val_metrics.get('correlation', 0.0)
            
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.best_val_correlation = val_correlation
                self.early_stopping_counter = 0
                self.save_checkpoint(epoch, val_loss, val_correlation)
                logger.info(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.6f}, ç›¸å…³ç³»æ•°: {val_correlation:.6f})")
            else:
                self.early_stopping_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if (self.config.EARLY_STOPPING_PATIENCE > 0 and 
                self.early_stopping_counter >= self.config.EARLY_STOPPING_PATIENCE):
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch}")
                break
        
        logger.info("æ”¹è¿›çš„Swin U-Netè®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        logger.info(f"æœ€ä½³éªŒè¯ç›¸å…³ç³»æ•°: {self.best_val_correlation:.6f}")
    
    def save_checkpoint(self, epoch: int, val_loss: float, val_correlation: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        config_dict = self.config.to_dict() if hasattr(self.config, 'to_dict') else vars(self.config).copy()
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'val_correlation': val_correlation,
            'config_dict': config_dict,
            'history': self.history
        }
        
        path = os.path.join(self.config.OUTPUT_DIR, f'best_swin_unet_{self.config.loss_config_name}.pth')
        torch.save(checkpoint, path)


def list_available_configs():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„Swin U-NetæŸå¤±é…ç½®"""
    print("=== å¯ç”¨çš„Swin U-NetæŸå¤±é…ç½® ===")
    for name, config in SWIN_LOSS_CONFIGS.items():
        use_tid = "âœ“" if config['use_tid_weights'] else "âœ—"
        grad_weight = config['gradient_loss_weight']
        print(f"\n{name}:")
        print(f"  åç§°: {config['name']}")
        print(f"  æè¿°: {config['description']}")
        print(f"  TIDæƒé‡: {use_tid}")
        print(f"  æ¢¯åº¦æŸå¤±æƒé‡: {grad_weight}")


def main():
    parser = argparse.ArgumentParser(description='æ”¹è¿›çš„Swin U-Netè®­ç»ƒ')
    parser.add_argument('--loss_config', type=str, default='tid_gradient',
                        choices=list(SWIN_LOSS_CONFIGS.keys()),
                        help='é€‰æ‹©æŸå¤±å‡½æ•°é…ç½®')
    parser.add_argument('--epochs', type=int, default=None,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--list_configs', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®')
    
    args = parser.parse_args()
    
    if args.list_configs:
        list_available_configs()
        return
    
    # åˆ›å»ºé…ç½®
    config = ImprovedSwinUNetConfig(args.loss_config)
    
    if args.epochs:
        config.NUM_EPOCHS = args.epochs
    if args.batch_size:
        config.BATCH_SIZE = args.batch_size
    
    logger.info("=== æ”¹è¿›çš„Swin U-Netæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ ===")
    logger.info(f"æŸå¤±é…ç½®: {config.loss_config_name} - {config.loss_config_description}")
    logger.info(f"è¿è¡Œè®¾å¤‡: {device}")
    logger.info(f"è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.RANDOM_SEED)
    np.random.seed(config.RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.RANDOM_SEED)
        torch.cuda.manual_seed_all(config.RANDOM_SEED)
    
    # æ•°æ®å‡†å¤‡
    data_prep = EnhancedDataPreparation(config)
    
    try:
        # åŠ è½½æ•°æ®
        features_ds, labels_da, tid_weights_da = data_prep.load_data()
        
        if features_ds is None:
            logger.error("æ•°æ®åŠ è½½å¤±è´¥")
            return
        
        # æå–æ•°æ®å—
        patches = data_prep.extract_patches(features_ds, labels_da, tid_weights_da)
        
        if not patches:
            logger.error("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„æ•°æ®å—ï¼")
            return
        
        logger.info(f"æå–åˆ° {len(patches)} ä¸ªæ•°æ®å—")
        
        # åˆ’åˆ†æ•°æ®é›†
        train_patches, val_patches, _ = data_prep.split_patches(patches)
        
        if not train_patches or not val_patches:
            logger.error("æ•°æ®é›†åˆ’åˆ†å¤±è´¥ï¼")
            return
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        train_dataset = EnhancedBathymetryDataset(train_patches, config, is_training=True)
        val_dataset = EnhancedBathymetryDataset(val_patches, config, is_training=False)
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=config.BATCH_SIZE, 
            shuffle=True,
            num_workers=config.NUM_WORKERS, 
            pin_memory=torch.cuda.is_available(), 
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=config.BATCH_SIZE, 
            shuffle=False,
            num_workers=config.NUM_WORKERS, 
            pin_memory=torch.cuda.is_available()
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = SwinUNet(config.SWIN_CONFIG)
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        logger.info(f"æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # è®­ç»ƒæ¨¡å‹
        trainer = ImprovedSwinUNetTrainer(model, config)
        trainer.train(train_loader, val_loader)
        
        # ä¿å­˜å®éªŒæ‘˜è¦
        experiment_summary = {
            'experiment_name': f'improved_swin_unet_{config.loss_config_name}',
            'loss_config_name': config.loss_config_name,
            'loss_config_description': config.loss_config_description,
            'model_type': 'Swin U-Net',
            'swin_config': config.SWIN_CONFIG,
            'loss_config': config.LOSS_CONFIG,
            'training_config': {
                'batch_size': config.BATCH_SIZE,
                'learning_rate': config.LEARNING_RATE,
                'num_epochs_run': len(trainer.history['train_loss_total']),
                'target_num_epochs': config.NUM_EPOCHS,
                'early_stopping_patience': config.EARLY_STOPPING_PATIENCE,
                'optimizer': 'AdamW',
                'scheduler': 'CosineAnnealingLR'
            },
            'best_val_loss': float(trainer.best_val_loss) if trainer.best_val_loss != float('inf') else None,
            'best_val_correlation': float(trainer.best_val_correlation),
            'total_params': int(total_params),
            'trainable_params': int(trainable_params),
            'final_epoch_completed': len(trainer.history['train_loss_total'])
        }
        
        summary_path = os.path.join(config.OUTPUT_DIR, f'improved_swin_summary_{config.loss_config_name}.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(experiment_summary, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(config.OUTPUT_DIR, f'improved_swin_history_{config.loss_config_name}.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(trainer.history, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æ”¹è¿›çš„Swin U-Netè®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {trainer.best_val_loss:.6f}")
        logger.info(f"ğŸ“Š æœ€ä½³éªŒè¯ç›¸å…³ç³»æ•°: {trainer.best_val_correlation:.6f}")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config.OUTPUT_DIR}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()