#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„Swin U-Netæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ - æ”¯æŒå¤šç§æŸå¤±å‡½æ•°é…ç½®

é›†æˆåŠŸèƒ½ï¼š
1. å®Œæ•´çš„Swin U-Netæ¨¡å‹å®ç°ï¼ˆTransformer+CNNæ··åˆæ¶æ„ï¼‰
2. å¤šç§é¢„è®¾æŸå¤±é…ç½®é€‰æ‹©ï¼ˆbaselineã€TIDåŠ æƒã€æ¢¯åº¦å¢å¼ºç­‰ï¼‰
3. å‘½ä»¤è¡Œæ¥å£æ”¯æŒ
4. TIDæƒé‡æ”¯æŒ
5. çª—å£æ³¨æ„åŠ›æœºåˆ¶å’Œç§»åŠ¨çª—å£

ä¸»è¦ç‰¹æ€§:
- åˆ†å±‚çš„Swin Transformerç¼–ç å™¨ - æ•è·å¤šå°ºåº¦ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡
- CNNè§£ç å™¨ - ç²¾ç¡®çš„ç©ºé—´å®šä½å’Œç»†èŠ‚æ¢å¤
- è·³è·ƒè¿æ¥å¢å¼º - ç»“åˆæµ…å±‚å’Œæ·±å±‚ç‰¹å¾
- å¯é…ç½®çš„æŸå¤±å‡½æ•°ç»„åˆ
- å®Œå…¨å…¼å®¹ç°æœ‰è®­ç»ƒæ¡†æ¶

ä½¿ç”¨æ–¹æ³•:
python swin_unet_complete.py --loss_config baseline
python swin_unet_complete.py --loss_config tid_weighted --epochs 100
python swin_unet_complete.py --loss_config tid_gradient --batch_size 4
python swin_unet_complete.py --list_configs

ä½œè€…: å‘¨ç‘å®¸
æ—¥æœŸ: 2025-05-26
ç‰ˆæœ¬: v2.0 (å®Œæ•´åˆå¹¶ç‰ˆæœ¬)
"""

import argparse
import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import json
import logging
import math
from datetime import datetime
from tqdm import tqdm
from typing import Optional, Tuple, List

# å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from enhanced_training_multi_loss import (
    EnhancedTrainingConfig, EnhancedDataPreparation,
    EnhancedBathymetryDataset, MultiLossFunction,
    device, setup_chinese_font
)

setup_chinese_font()
logger = logging.getLogger('swin_unet_complete')


# ==================== æŸå¤±å‡½æ•°é…ç½®é¢„è®¾ ====================
SWIN_LOSS_CONFIGS = {
    'baseline': {
        'name': 'Baseline',
        'loss_combination': 'baseline',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.0,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': False,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.0,
        'description': 'æ ‡å‡†MSEæŸå¤±ï¼Œä¸ä½¿ç”¨æƒé‡å’Œé¢å¤–æŸå¤±'
    },
    
    'tid_weighted': {
        'name': 'TIDåŠ æƒ',
        'loss_combination': 'weighted_mse',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.0,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.0,
        'description': 'ä½¿ç”¨TIDæƒé‡çš„MSEæŸå¤±ï¼Œé‡ç‚¹å…³æ³¨é«˜è´¨é‡åŒºåŸŸ'
    },
    
    'tid_gradient': {
        'name': 'TIDæƒé‡+æ¢¯åº¦',
        'loss_combination': 'weighted_mse_grad',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.3,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.0,
        'description': 'TIDæƒé‡MSE + æ¢¯åº¦æŸå¤±ï¼Œå¢å¼ºç»†èŠ‚å’Œè¾¹ç¼˜'
    },
    
    'enhanced_gradient': {
        'name': 'å¢å¼ºæ¢¯åº¦',
        'loss_combination': 'weighted_mse_grad',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.5,
        'spectral_loss_weight': 0.0,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.2,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.5,
        'description': 'å¢å¼ºçš„æ¢¯åº¦æŸå¤±æƒé‡ï¼Œæœ€å¤§åŒ–ç»†èŠ‚æ¢å¤èƒ½åŠ›'
    },
    
    'all_combined': {
        'name': 'å…¨æŸå¤±ç»„åˆ',
        'loss_combination': 'all',
        'pixel_loss_weight': 1.0,
        'gradient_loss_weight': 0.5,
        'spectral_loss_weight': 0.3,
        'pixel_loss_type': 'mse',
        'use_tid_weights': True,
        'tid_weight_scale': 1.0,
        'gradient_method': 'sobel',
        'spectral_focus_high_freq': True,
        'high_freq_weight_factor': 2.5,
        'description': 'ä½¿ç”¨æ‰€æœ‰æŸå¤±ç»„åˆï¼šMSE + æ¢¯åº¦ + é¢‘è°±ï¼Œå…¨é¢ä¼˜åŒ–'
    }
}


# ==================== Swin Transformer æ ¸å¿ƒç»„ä»¶ ====================

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample"""
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample"""
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


def window_partition(x, window_size: int):
    """
    å°†ç‰¹å¾å›¾åˆ†å‰²ä¸ºçª—å£
    Args:
        x: (B, H, W, C)
        window_size: window size
    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    # ç¡®ä¿ H å’Œ W è‡³å°‘ç­‰äº window_size
    if not (H >= window_size and W >= window_size):
        logger.warning(f"window_partition called with H={H} or W={W} < window_size={window_size}. Errors may occur.")

    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size: int, H: int, W: int):
    """
    å°†çª—å£åˆå¹¶å›ç‰¹å¾å›¾
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size: Window size
        H: Height of image
        W: Width of image
    Returns:
        x: (B, H, W, C)
    """
    num_windows_h = H // window_size
    num_windows_w = W // window_size
    if num_windows_h == 0 or num_windows_w == 0:
        logger.error(f"window_reverse: num_windows_h or num_windows_w is 0. H={H}, W={W}, ws={window_size}")
        raise ValueError(f"window_reverse: H ({H}) or W ({W}) is too small for window_size ({window_size}) leading to zero windows.")

    B = int(windows.shape[0] / (num_windows_h * num_windows_w))
    x = windows.view(B, num_windows_h, num_windows_w, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    """Window based multi-head self attention (W-MSA) module with relative position bias"""

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.dim = dim
        if isinstance(window_size, int):
            self.window_size_tuple = (window_size, window_size)
        else:
            self.window_size_tuple = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        # å®šä¹‰ç›¸å¯¹ä½ç½®åç½®è¡¨
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * self.window_size_tuple[0] - 1) * (2 * self.window_size_tuple[1] - 1), num_heads))

        # è·å–æ¯ä¸ªtokençš„ç›¸å¯¹ä½ç½®ç´¢å¼•
        coords_h = torch.arange(self.window_size_tuple[0])
        coords_w = torch.arange(self.window_size_tuple[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size_tuple[0] - 1
        relative_coords[:, :, 1] += self.window_size_tuple[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size_tuple[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size_tuple[0] * self.window_size_tuple[1], self.window_size_tuple[0] * self.window_size_tuple[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = F.softmax(attn, dim=-1)
        else:
            attn = F.softmax(attn, dim=-1)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class SwinTransformerBlock(nn.Module):
    """Swin Transformer Block"""

    def __init__(self, dim, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        self.H = None  # Set by BasicLayer
        self.W = None  # Set by BasicLayer

        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(
            dim, window_size=self.window_size, num_heads=num_heads,
            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(drop),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(drop)
        )

    def forward(self, x, attn_mask_input):
        """
        Args:
            x: input features with shape of (B, L, C)
            attn_mask_input: attention mask
        """
        H_in, W_in = self.H, self.W
        B, L, C = x.shape
        assert L == H_in * W_in, f"Input L={L} != H_in*W_in={H_in*W_in}"

        shortcut = x
        x_norm1 = self.norm1(x)
        x_spatial = x_norm1.view(B, H_in, W_in, C)

        # å¦‚æœéœ€è¦å¡«å……ä»¥æ»¡è¶³çª—å£å¤§å°è¦æ±‚
        H_eff, W_eff = H_in, W_in
        pad_b_op, pad_r_op = 0, 0
        needs_padding = False

        if H_in < self.window_size:
            pad_b_op = self.window_size - H_in
            H_eff = self.window_size
            needs_padding = True
        if W_in < self.window_size:
            pad_r_op = self.window_size - W_in
            W_eff = self.window_size
            needs_padding = True

        x_to_shift = x_spatial
        if needs_padding:
            x_permuted = x_spatial.permute(0, 3, 1, 2)  # B, C, H_in, W_in
            x_padded_permuted = F.pad(x_permuted, (0, pad_r_op, 0, pad_b_op))  # Pad R, B
            x_to_shift = x_padded_permuted.permute(0, 2, 3, 1)  # B, H_eff, W_eff, C

        # å¾ªç¯ç§»ä½
        if self.shift_size > 0:
            shifted_x = torch.roll(x_to_shift, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x_to_shift

        # åˆ†å‰²çª—å£
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=attn_mask_input)

        # åˆå¹¶çª—å£
        attn_windows_merged = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x_reverted = window_reverse(attn_windows_merged, self.window_size, H_eff, W_eff)

        # åå‘å¾ªç¯ç§»ä½
        if self.shift_size > 0:
            x_rolled_back = torch.roll(shifted_x_reverted, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x_rolled_back = shifted_x_reverted

        # å¦‚æœè¿›è¡Œäº†å¡«å……ï¼Œåˆ™è£å‰ªå›åŸå§‹å¤§å°
        if needs_padding:
            x_unpadded_spatial = x_rolled_back[:, :H_in, :W_in, :].contiguous()
        else:
            x_unpadded_spatial = x_rolled_back

        x_final_seq = x_unpadded_spatial.view(B, H_in * W_in, C)

        # æ®‹å·®è¿æ¥å’ŒFFN
        x = shortcut + self.drop_path(x_final_seq)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x


class PatchMerging(nn.Module):
    """Patch Merging Layer"""

    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)

    def forward(self, x, H: int, W: int):
        """
        Args:
            x: input features with shape of (B, L, C)
            H, W: spatial resolution of the input feature.
        """
        B, L, C = x.shape
        assert L == H * W, "PatchMerging: Input L mismatch"
        assert C == self.dim, "PatchMerging: Input C mismatch"

        x_spatial = x.view(B, H, W, C)

        # å¦‚æœHæˆ–Wæ˜¯å¥‡æ•°ï¼Œè¿›è¡Œå¡«å……
        H_padded, W_padded = H, W
        if H % 2 == 1:
            H_padded += 1
        if W % 2 == 1:
            W_padded += 1

        x_to_sample = x_spatial
        if H_padded != H or W_padded != W:
            x_perm = x_spatial.permute(0, 3, 1, 2)  # B, C, H, W
            x_padded_perm = F.pad(x_perm, (0, W_padded - W, 0, H_padded - H))  # pad right, bottom
            x_to_sample = x_padded_perm.permute(0, 2, 3, 1)  # B, H_padded, W_padded, C

        # ä¸‹é‡‡æ ·
        x0 = x_to_sample[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x_to_sample[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x_to_sample[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x_to_sample[:, 1::2, 1::2, :]  # B H/2 W/2 C

        merged_x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        merged_x_seq = merged_x.view(B, -1, 4 * C)
        merged_x_seq_norm = self.norm(merged_x_seq)
        output_seq = self.reduction(merged_x_seq_norm)
        return output_seq


class BasicLayer(nn.Module):
    """A basic Swin Transformer layer for one stage"""

    def __init__(self, dim, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,
                 drop_path=0., downsample=None):
        super().__init__()
        self.dim = dim
        self.depth = depth
        self.window_size = window_size
        self.shift_size = window_size // 2

        # æ„å»ºblocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=0 if (i % 2 == 0) else self.shift_size,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(dim=dim)
        else:
            self.downsample = None

    def forward(self, x, H, W):
        """Forward function"""
        attn_mask = None
        if self.shift_size > 0:
            # è®¡ç®—SW-MSAçš„æ³¨æ„åŠ›æ©ç 
            Hp = int(np.ceil(H / self.window_size)) * self.window_size
            Wp = int(np.ceil(W / self.window_size)) * self.window_size
            img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h_s in h_slices:
                for w_s in w_slices:
                    img_mask[:, h_s, w_s, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        x_processed_blocks = x
        for blk in self.blocks:
            blk.H, blk.W = H, W
            x_processed_blocks = blk(x_processed_blocks, attn_mask)

        if self.downsample is not None:
            x_next_layer_input = self.downsample(x_processed_blocks, H, W)
            H_next = (H + 1) // 2
            W_next = (W + 1) // 2
            return x_processed_blocks, H, W, x_next_layer_input, H_next, W_next
        else:
            return x_processed_blocks, H, W, x_processed_blocks, H, W


class DecoderBlock(nn.Module):
    """è§£ç å™¨å—"""

    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()
        upsampled_x_channels = in_channels // 2
        self.upsample = nn.ConvTranspose2d(in_channels, upsampled_x_channels, kernel_size=2, stride=2)

        conv_block_input_ch = upsampled_x_channels
        if skip_channels is not None and skip_channels > 0:
            # å°†è·³è·ƒè¿æ¥ç‰¹å¾æŠ•å½±åˆ°ä¸ä¸Šé‡‡æ ·ç‰¹å¾ç›¸åŒçš„é€šé“æ•°
            self.skip_projection = nn.Conv2d(skip_channels, upsampled_x_channels, kernel_size=1, bias=False)
            conv_block_input_ch += upsampled_x_channels  # æ‹¼æ¥åçš„æ€»é€šé“æ•°
        else:
            self.skip_projection = None

        self.conv_block = nn.Sequential(
            nn.Conv2d(conv_block_input_ch, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x_from_prev_decoder, skip_from_encoder=None):
        x_upsampled = self.upsample(x_from_prev_decoder)

        if skip_from_encoder is not None and self.skip_projection is not None:
            skip_projected = self.skip_projection(skip_from_encoder)
            # ç¡®ä¿ç©ºé—´å°ºå¯¸åŒ¹é…
            if x_upsampled.shape[2:] != skip_projected.shape[2:]:
                skip_projected = F.interpolate(
                    skip_projected, size=x_upsampled.shape[2:],
                    mode='bilinear', align_corners=False)
            x_to_conv_block = torch.cat([x_upsampled, skip_projected], dim=1)
        else:
            x_to_conv_block = x_upsampled

        output = self.conv_block(x_to_conv_block)
        return output


# ==================== Swin U-Net æ¨¡å‹ ====================

class SwinUNet(nn.Module):
    """Swin U-Netæ¨¡å‹"""

    def __init__(self, config_dict):
        super().__init__()
        self.patch_size_val = config_dict['patch_size']
        self.in_chans = config_dict['in_chans']
        self.embed_dim_val = config_dict['embed_dim']
        self.depths = config_dict['depths']
        self.num_heads = config_dict['num_heads']
        self.window_size = config_dict['window_size']
        self.mlp_ratio = config_dict['mlp_ratio']
        self.drop_rate = config_dict['drop_rate']
        self.attn_drop_rate = config_dict['attn_drop_rate']
        self.drop_path_rate = config_dict['drop_path_rate']
        self.num_layers = len(self.depths)
        self.patch_norm = True

        # Patch embedding
        self.patch_embed = nn.Conv2d(self.in_chans, self.embed_dim_val,
                                     kernel_size=self.patch_size_val, stride=self.patch_size_val)
        if self.patch_norm:
            self.norm_embed = nn.LayerNorm(self.embed_dim_val)
        self.pos_drop = nn.Dropout(p=self.drop_rate)

        # Stochastic depth
        dpr = [x.item() for x in torch.linspace(0, self.drop_path_rate, sum(self.depths))]

        # æ„å»ºç¼–ç å™¨å±‚
        self.layers = nn.ModuleList()
        current_dim_encoder = self.embed_dim_val
        for i_layer in range(self.num_layers):
            layer = BasicLayer(
                dim=current_dim_encoder,
                depth=self.depths[i_layer],
                num_heads=self.num_heads[i_layer],
                window_size=self.window_size,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=True,
                drop=self.drop_rate,
                attn_drop=self.attn_drop_rate,
                drop_path=dpr[sum(self.depths[:i_layer]):sum(self.depths[:i_layer + 1])],
                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None)
            self.layers.append(layer)
            if i_layer < self.num_layers - 1:
                current_dim_encoder = int(current_dim_encoder * 2)

        # æ„å»ºè§£ç å™¨
        self.decoder_channels_conf = config_dict['decoder_channels']
        self.decoders = nn.ModuleList()
        bottleneck_dim = current_dim_encoder  # æœ€åä¸€ä¸ªç¼–ç å™¨å±‚çš„ç»´åº¦

        decoder_input_dim = bottleneck_dim
        for i in range(len(self.decoder_channels_conf)):
            skip_encoder_stage_idx = self.num_layers - 2 - i
            skip_channels_dim = None
            if skip_encoder_stage_idx >= 0:
                skip_channels_dim = int(self.embed_dim_val * (2 ** skip_encoder_stage_idx))

            decoder = DecoderBlock(
                in_channels=decoder_input_dim,
                skip_channels=skip_channels_dim,
                out_channels=self.decoder_channels_conf[i])
            self.decoders.append(decoder)
            decoder_input_dim = self.decoder_channels_conf[i]

        # æœ€ç»ˆè¾“å‡ºå±‚
        self.final_conv = nn.Sequential(
            nn.Conv2d(self.decoder_channels_conf[-1], 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1)
        )

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
            if m.weight is not None:
                nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            if m.weight is not None:
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward_features(self, x_in_spatial):
        """ç‰¹å¾æå–"""
        x_after_embed = self.patch_embed(x_in_spatial)  # (B, embed_dim, H_p, W_p)
        H_curr, W_curr = x_after_embed.size(2), x_after_embed.size(3)

        x_seq = x_after_embed.flatten(2).transpose(1, 2)  # (B, L_p, embed_dim)
        if self.patch_norm:
            x_seq = self.norm_embed(x_seq)
        x_seq = self.pos_drop(x_seq)

        skip_connections_spatial_list = []

        for layer_module in self.layers:
            x_block_out_seq, H_block_out, W_block_out, \
            x_next_in_seq, H_next_in, W_next_in = layer_module(x_seq, H_curr, W_curr)

            # ä¿å­˜è·³è·ƒè¿æ¥
            skip_C = x_block_out_seq.size(-1)
            skip_spatial = x_block_out_seq.transpose(1, 2).contiguous().view(-1, skip_C, H_curr, W_curr)
            skip_connections_spatial_list.append(skip_spatial)

            x_seq = x_next_in_seq
            H_curr = H_next_in
            W_curr = W_next_in

        return x_seq, skip_connections_spatial_list, H_curr, W_curr

    def forward(self, x_input_to_model):
        """å‰å‘ä¼ æ’­"""
        bottleneck_seq, skips_spatial, H_bottle, W_bottle = self.forward_features(x_input_to_model)

        B, L_bottle, C_bottle = bottleneck_seq.shape
        if H_bottle == 0 or W_bottle == 0:
            logger.error(f"Zero dimension in bottleneck: H={H_bottle}, W={W_bottle}. Input: {x_input_to_model.shape}")
            # è¿”å›é›¶å¼ é‡ä½œä¸ºfallback
            dummy_output_shape = (B, 1, x_input_to_model.shape[2], x_input_to_model.shape[3])
            return torch.zeros(dummy_output_shape, device=bottleneck_seq.device, dtype=bottleneck_seq.dtype)

        assert L_bottle > 0 or (H_bottle == 0 and W_bottle == 0), "Bottleneck L is 0 but H/W are not."

        x_dec_in_spatial = bottleneck_seq.transpose(1, 2).contiguous().view(B, C_bottle, H_bottle, W_bottle)

        for i, decoder_module in enumerate(self.decoders):
            skip_idx_for_decoder = self.num_layers - 2 - i
            skip_to_use = None
            if skip_idx_for_decoder >= 0:
                if skip_idx_for_decoder < len(skips_spatial):
                    skip_to_use = skips_spatial[skip_idx_for_decoder]
                else:
                    logger.warning(f"Decoder {i}: skip_idx_for_decoder {skip_idx_for_decoder} out of bounds for skips_spatial (len {len(skips_spatial)})")

            x_dec_in_spatial = decoder_module(x_dec_in_spatial, skip_to_use)

        output_final = self.final_conv(x_dec_in_spatial)
        return output_final


# ==================== é…ç½®ç±» ====================

class SwinUNetConfig(EnhancedTrainingConfig):
    """å®Œæ•´çš„Swin U-Neté…ç½®ç±» - æ”¯æŒæŸå¤±å‡½æ•°é€‰æ‹©"""

    def __init__(self, loss_config_name='tid_gradient'):
        super().__init__()

        # éªŒè¯æŸå¤±é…ç½®
        if loss_config_name not in SWIN_LOSS_CONFIGS:
            available = list(SWIN_LOSS_CONFIGS.keys())
            raise ValueError(f"æœªçŸ¥çš„æŸå¤±é…ç½®: {loss_config_name}. å¯ç”¨é…ç½®: {available}")

        # åŸºç¡€é…ç½®
        self.OUTPUT_DIR = f"/mnt/data5/06-Projects/05-unet/output/3-swin_{loss_config_name}_outputs_lt135km"

        # åº”ç”¨é€‰æ‹©çš„æŸå¤±é…ç½®
        loss_config = SWIN_LOSS_CONFIGS[loss_config_name]
        self.LOSS_CONFIG = loss_config.copy()

        # Swin Transformerç‰¹å®šå‚æ•°
        self.SWIN_CONFIG = {
            'patch_size': 4,
            'in_chans': 4,
            'embed_dim': 96,
            'depths': [2, 2, 6, 2],
            'num_heads': [3, 6, 12, 24],
            'window_size': 8,
            'mlp_ratio': 4.0,
            'drop_rate': 0.0,
            'attn_drop_rate': 0.0,
            'drop_path_rate': 0.1,
            'use_checkpoint': False,
            'decoder_channels': [512, 256, 128, 64],
            'skip_connection_type': 'concat',
            'final_upsample': 'expand_first',
        }

        # è®­ç»ƒå‚æ•°ä¼˜åŒ–
        self.BATCH_SIZE = 6  # é€‚åˆSwin U-Netçš„æ‰¹æ¬¡å¤§å°
        self.LEARNING_RATE = 1e-4
        self.NUM_EPOCHS = 150
        self.WEIGHT_DECAY = 0.05
        self.EARLY_STOPPING_PATIENCE = 20

        # ä¿å­˜é…ç½®ä¿¡æ¯
        self.loss_config_name = loss_config_name
        self.loss_config_description = loss_config['description']

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸ç”¨äºä¿å­˜"""
        return {
            key: value for key, value in self.__dict__.items()
            if not key.startswith('__') and not callable(value)
        }


# ==================== è®­ç»ƒå™¨ç±» ====================

class SwinUNetTrainer:
    """å®Œæ•´çš„Swin U-Netè®­ç»ƒå™¨"""

    def __init__(self, model: nn.Module, config: SwinUNetConfig):
        self.model = model.to(device)
        self.config = config

        # ä½¿ç”¨ç°æœ‰çš„å¤šé‡æŸå¤±å‡½æ•°
        self.criterion = MultiLossFunction(config.LOSS_CONFIG)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )

        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.NUM_EPOCHS,
            eta_min=config.LEARNING_RATE * 0.01
        )

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss_total': [],
            'train_loss_pixel': [],
            'train_loss_gradient': [],
            'train_loss_spectral': [],
            'val_loss_total': [],
            'val_loss_pixel': [],
            'val_loss_gradient': [],
            'val_loss_spectral': [],
            'val_correlation': [],
            'learning_rates': []
        }

        self.best_val_loss = float('inf')
        self.best_val_correlation = -1.0
        self.early_stopping_counter = 0

    def train_epoch(self, train_loader: DataLoader, epoch: int) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        epoch_losses = {
            'total': 0.0,
            'pixel': 0.0,
            'gradient': 0.0,
            'spectral': 0.0
        }

        progress_bar = tqdm(train_loader, desc=f'è®­ç»ƒ Epoch {epoch}', leave=False)

        for batch_idx, batch_data in enumerate(progress_bar):
            # è·å–æ•°æ®
            features = batch_data[0].to(device)
            labels = batch_data[1].to(device)
            tid_weights = batch_data[2].to(device) if len(batch_data) == 3 and self.config.LOSS_CONFIG['use_tid_weights'] else None

            # å‰å‘ä¼ æ’­
            outputs = self.model(features)

            # ç¡®ä¿è¾“å‡ºå°ºå¯¸åŒ¹é…
            if outputs.shape[-2:] != labels.shape[-2:]:
                outputs = F.interpolate(outputs, size=labels.shape[-2:],
                                        mode='bilinear', align_corners=False)

            # è®¡ç®—æŸå¤±
            loss, loss_components = self.criterion(outputs, labels, tid_weights)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()

            # è®°å½•æŸå¤±
            for key in epoch_losses:
                if key in loss_components:
                    value = loss_components[key]
                    if isinstance(value, torch.Tensor):
                        epoch_losses[key] += value.item()
                    elif isinstance(value, (int, float)):
                        epoch_losses[key] += value

            # æ›´æ–°è¿›åº¦æ¡
            postfix_dict = {}
            for k, v in loss_components.items():
                if k in ['total', 'pixel', 'gradient']:
                    if isinstance(v, torch.Tensor):
                        postfix_dict[k] = f'{v.item():.4f}'
                    elif isinstance(v, (int, float)):
                        postfix_dict[k] = f'{v:.4f}'
            progress_bar.set_postfix(postfix_dict)

        # è®¡ç®—å¹³å‡æŸå¤±
        n_batches = len(train_loader)
        return {key: val / n_batches if n_batches > 0 else 0.0 for key, val in epoch_losses.items()}

    def validate(self, val_loader: DataLoader) -> dict:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()

        epoch_losses = {
            'total': 0.0,
            'pixel': 0.0,
            'gradient': 0.0,
            'spectral': 0.0
        }

        all_predictions = []
        all_targets = []

        with torch.no_grad():
            for batch_data in tqdm(val_loader, desc='éªŒè¯ä¸­', leave=False):
                features = batch_data[0].to(device)
                labels = batch_data[1].to(device)
                tid_weights = batch_data[2].to(device) if len(batch_data) == 3 and self.config.LOSS_CONFIG['use_tid_weights'] else None

                # é¢„æµ‹
                outputs = self.model(features)

                # ç¡®ä¿å°ºå¯¸åŒ¹é…
                if outputs.shape[-2:] != labels.shape[-2:]:
                    outputs = F.interpolate(outputs, size=labels.shape[-2:],
                                            mode='bilinear', align_corners=False)

                # è®¡ç®—æŸå¤±
                loss, loss_components = self.criterion(outputs, labels, tid_weights)

                # è®°å½•æŸå¤±
                for key in epoch_losses:
                    if key in loss_components:
                        value = loss_components[key]
                        if isinstance(value, torch.Tensor):
                            epoch_losses[key] += value.item()
                        elif isinstance(value, (int, float)):
                            epoch_losses[key] += value

                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions.append(outputs.cpu().numpy())
                all_targets.append(labels.cpu().numpy())

        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = 0.0
        if all_predictions and all_targets:
            predictions = np.concatenate(all_predictions, axis=0)
            targets = np.concatenate(all_targets, axis=0)

            pred_flat = predictions.flatten()
            target_flat = targets.flatten()

            if len(pred_flat) > 1 and len(target_flat) > 1:
                correlation = np.corrcoef(pred_flat, target_flat)[0, 1]
                if np.isnan(correlation):
                    correlation = 0.0

        # è®¡ç®—å¹³å‡æŸå¤±
        n_batches = len(val_loader)
        results = {key: val / n_batches if n_batches > 0 else 0.0 for key, val in epoch_losses.items()}
        results['correlation'] = correlation

        return results

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info(f"å¼€å§‹å®Œæ•´çš„Swin U-Netè®­ç»ƒ")
        logger.info(f"æŸå¤±é…ç½®: {self.config.loss_config_name} - {self.config.loss_config_description}")

        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)

        for epoch in range(1, self.config.NUM_EPOCHS + 1):
            logger.info(f"\nEpoch {epoch}/{self.config.NUM_EPOCHS}")

            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, epoch)

            # éªŒè¯
            val_metrics = self.validate(val_loader)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']

            # æ›´æ–°å†å²è®°å½•
            self.history['train_loss_total'].append(train_metrics.get('total', 0.0))
            self.history['train_loss_pixel'].append(train_metrics.get('pixel', 0.0))
            self.history['train_loss_gradient'].append(train_metrics.get('gradient', 0.0))
            self.history['train_loss_spectral'].append(train_metrics.get('spectral', 0.0))

            self.history['val_loss_total'].append(val_metrics.get('total', 0.0))
            self.history['val_loss_pixel'].append(val_metrics.get('pixel', 0.0))
            self.history['val_loss_gradient'].append(val_metrics.get('gradient', 0.0))
            self.history['val_loss_spectral'].append(val_metrics.get('spectral', 0.0))

            self.history['val_correlation'].append(val_metrics.get('correlation', 0.0))
            self.history['learning_rates'].append(current_lr)

            # æ‰“å°æŒ‡æ ‡
            train_log = f"æ€»æŸå¤±: {train_metrics.get('total', 0):.6f}"
            if train_metrics.get('pixel', 0) > 0:
                train_log += f", åƒç´ : {train_metrics.get('pixel', 0):.6f}"
            if train_metrics.get('gradient', 0) > 0:
                train_log += f", æ¢¯åº¦: {train_metrics.get('gradient', 0):.6f}"

            val_log = f"æ€»æŸå¤±: {val_metrics.get('total', 0):.6f}, ç›¸å…³ç³»æ•°: {val_metrics.get('correlation', 0):.6f}"

            logger.info(f"è®­ç»ƒ - {train_log}")
            logger.info(f"éªŒè¯ - {val_log}")
            logger.info(f"å­¦ä¹ ç‡: {current_lr:.6e}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
            val_loss = val_metrics.get('total', float('inf'))
            val_correlation = val_metrics.get('correlation', 0.0)

            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.best_val_correlation = val_correlation
                self.early_stopping_counter = 0
                self.save_checkpoint(epoch, val_loss, val_correlation)
                logger.info(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.6f}, ç›¸å…³ç³»æ•°: {val_correlation:.6f})")
            else:
                self.early_stopping_counter += 1

            # æ—©åœæ£€æŸ¥
            if (self.config.EARLY_STOPPING_PATIENCE > 0 and
                    self.early_stopping_counter >= self.config.EARLY_STOPPING_PATIENCE):
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch}")
                break

        logger.info("å®Œæ•´çš„Swin U-Netè®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        logger.info(f"æœ€ä½³éªŒè¯ç›¸å…³ç³»æ•°: {self.best_val_correlation:.6f}")

    def save_checkpoint(self, epoch: int, val_loss: float, val_correlation: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        config_dict = self.config.to_dict() if hasattr(self.config, 'to_dict') else vars(self.config).copy()

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'val_correlation': val_correlation,
            'config_dict': config_dict,
            'history': self.history
        }

        path = os.path.join(self.config.OUTPUT_DIR, f'best_swin_unet_{self.config.loss_config_name}.pth')
        torch.save(checkpoint, path)


# ==================== å·¥å…·å‡½æ•° ====================

def list_available_configs():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„Swin U-NetæŸå¤±é…ç½®"""
    print("=== å¯ç”¨çš„Swin U-NetæŸå¤±é…ç½® ===")
    for name, config in SWIN_LOSS_CONFIGS.items():
        use_tid = "âœ“" if config['use_tid_weights'] else "âœ—"
        grad_weight = config['gradient_loss_weight']
        spectral_weight = config['spectral_loss_weight']
        print(f"\n{name}:")
        print(f"  åç§°: {config['name']}")
        print(f"  æè¿°: {config['description']}")
        print(f"  TIDæƒé‡: {use_tid}")
        print(f"  æ¢¯åº¦æŸå¤±æƒé‡: {grad_weight}")
        print(f"  é¢‘è°±æŸå¤±æƒé‡: {spectral_weight}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description='å®Œæ•´çš„Swin U-Netæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ')
    parser.add_argument('--loss_config', type=str, default='tid_gradient',
                        choices=list(SWIN_LOSS_CONFIGS.keys()),
                        help='é€‰æ‹©æŸå¤±å‡½æ•°é…ç½®')
    parser.add_argument('--epochs', type=int, default=None,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=None,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=None,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--list_configs', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®')

    args = parser.parse_args()

    if args.list_configs:
        list_available_configs()
        return

    # åˆ›å»ºé…ç½®
    config = SwinUNetConfig(args.loss_config)

    if args.epochs:
        config.NUM_EPOCHS = args.epochs
    if args.batch_size:
        config.BATCH_SIZE = args.batch_size
    if args.learning_rate:
        config.LEARNING_RATE = args.learning_rate

    logger.info("=== å®Œæ•´çš„Swin U-Netæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ ===")
    logger.info(f"æŸå¤±é…ç½®: {config.loss_config_name} - {config.loss_config_description}")
    logger.info(f"è¿è¡Œè®¾å¤‡: {device}")
    logger.info(f"è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.RANDOM_SEED)
    np.random.seed(config.RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.RANDOM_SEED)
        torch.cuda.manual_seed_all(config.RANDOM_SEED)

    # æ•°æ®å‡†å¤‡
    data_prep = EnhancedDataPreparation(config)

    try:
        # åŠ è½½æ•°æ®
        features_ds, labels_da, tid_weights_da = data_prep.load_data()

        if features_ds is None:
            logger.error("æ•°æ®åŠ è½½å¤±è´¥")
            return

        # æå–æ•°æ®å—
        patches = data_prep.extract_patches(features_ds, labels_da, tid_weights_da)

        if not patches:
            logger.error("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„æ•°æ®å—ï¼")
            return

        logger.info(f"æå–åˆ° {len(patches)} ä¸ªæ•°æ®å—")

        # åˆ’åˆ†æ•°æ®é›†
        train_patches, val_patches, _ = data_prep.split_patches(patches)

        if not train_patches or not val_patches:
            logger.error("æ•°æ®é›†åˆ’åˆ†å¤±è´¥ï¼")
            return

        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        train_dataset = EnhancedBathymetryDataset(train_patches, config, is_training=True)
        val_dataset = EnhancedBathymetryDataset(val_patches, config, is_training=False)

        train_loader = DataLoader(
            train_dataset,
            batch_size=config.BATCH_SIZE,
            shuffle=True,
            num_workers=config.NUM_WORKERS,
            pin_memory=torch.cuda.is_available(),
            drop_last=True
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=config.BATCH_SIZE,
            shuffle=False,
            num_workers=config.NUM_WORKERS,
            pin_memory=torch.cuda.is_available()
        )

        # åˆ›å»ºæ¨¡å‹
        model = SwinUNet(config.SWIN_CONFIG)

        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        logger.info(f"æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        # è®­ç»ƒæ¨¡å‹
        trainer = SwinUNetTrainer(model, config)
        trainer.train(train_loader, val_loader)

        # ä¿å­˜å®éªŒæ‘˜è¦
        experiment_summary = {
            'experiment_name': f'swin_unet_complete_{config.loss_config_name}',
            'loss_config_name': config.loss_config_name,
            'loss_config_description': config.loss_config_description,
            'model_type': 'Swin U-Net',
            'swin_config': config.SWIN_CONFIG,
            'loss_config': config.LOSS_CONFIG,
            'training_config': {
                'batch_size': config.BATCH_SIZE,
                'learning_rate': config.LEARNING_RATE,
                'num_epochs_run': len(trainer.history['train_loss_total']),
                'target_num_epochs': config.NUM_EPOCHS,
                'early_stopping_patience': config.EARLY_STOPPING_PATIENCE,
                'optimizer': 'AdamW',
                'scheduler': 'CosineAnnealingLR'
            },
            'best_val_loss': float(trainer.best_val_loss) if trainer.best_val_loss != float('inf') else None,
            'best_val_correlation': float(trainer.best_val_correlation),
            'total_params': int(total_params),
            'trainable_params': int(trainable_params),
            'final_epoch_completed': len(trainer.history['train_loss_total'])
        }

        summary_path = os.path.join(config.OUTPUT_DIR, f'swin_complete_summary_{config.loss_config_name}.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(experiment_summary, f, indent=2, ensure_ascii=False)

        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(config.OUTPUT_DIR, f'swin_complete_history_{config.loss_config_name}.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(trainer.history, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… å®Œæ•´çš„Swin U-Netè®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {trainer.best_val_loss:.6f}")
        logger.info(f"ğŸ“Š æœ€ä½³éªŒè¯ç›¸å…³ç³»æ•°: {trainer.best_val_correlation:.6f}")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config.OUTPUT_DIR}")

    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()
