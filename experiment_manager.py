#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®éªŒç®¡ç†å™¨ - ç³»ç»ŸåŒ–çš„æŸå¤±å‡½æ•°å¯¹æ¯”å®éªŒ

è¿™ä¸ªè„šæœ¬æä¾›äº†å®Œæ•´çš„å®éªŒç®¡ç†åŠŸèƒ½ï¼š
1. é€æ­¥æ‰§è¡Œå®éªŒé…ç½®
2. å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦
3. è‡ªåŠ¨æ”¶é›†å’Œå¯¹æ¯”ç»“æœ
4. ç”Ÿæˆåˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
python experiment_manager.py --mode sequential --epochs 20  # é€æ­¥è¿è¡Œå®éªŒ
python experiment_manager.py --mode analyze                 # åˆ†æå·²æœ‰ç»“æœ
python experiment_manager.py --mode quick_test             # å¿«é€Ÿæµ‹è¯•æ‰€æœ‰é…ç½®

ä½œè€…: å‘¨ç‘å®¸
æ—¥æœŸ: 2025-05-25
"""

import os
import sys
import argparse
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import subprocess
import time
import logging
from typing import Dict, List, Tuple
import glob
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('experiment_manager.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('experiment_manager')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“  
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class ExperimentManager:
    """å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, base_dir="/mnt/data5/06-Projects/05-unet"):
        self.base_dir = base_dir
        self.output_base = os.path.join(base_dir, "output")
        self.results_dir = os.path.join(base_dir, "experiment_results")
        self.analysis_dir = os.path.join(base_dir, "analysis_results")
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.results_dir, self.analysis_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # å®éªŒé…ç½®å®šä¹‰ï¼ˆæŒ‰æ¨èé¡ºåºï¼‰
        self.experiment_configs = [
            ('baseline', 'åŸºçº¿å®éªŒ - æ ‡å‡†MSE', 'baseline'),
            ('weighted_mse', 'TIDæƒé‡MSE', 'weighted_mse'),
            ('mse_grad', 'MSE + æ¢¯åº¦æŸå¤±', 'weighted_mse_grad'),
            ('mse_fft', 'MSE + é¢‘è°±æŸå¤±', 'weighted_mse_fft'),
            ('all_combined', 'å…¨æŸå¤±ç»„åˆ', 'all'),
        ]
        
        # å®éªŒçŠ¶æ€è·Ÿè¸ª
        self.experiment_status = {}
        self.load_experiment_status()
    
    def load_experiment_status(self):
        """åŠ è½½å®éªŒçŠ¶æ€"""
        status_file = os.path.join(self.results_dir, 'experiment_status.json')
        if os.path.exists(status_file):
            with open(status_file, 'r') as f:
                self.experiment_status = json.load(f)
        else:
            self.experiment_status = {config[0]: 'not_started' for config in self.experiment_configs}
    
    def save_experiment_status(self):
        """ä¿å­˜å®éªŒçŠ¶æ€"""
        status_file = os.path.join(self.results_dir, 'experiment_status.json')
        with open(status_file, 'w') as f:
            json.dump(self.experiment_status, f, indent=2)
    
    def run_single_experiment(self, config_name: str, epochs: int = 50, force_restart: bool = False) -> bool:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        if not force_restart and self.experiment_status.get(config_name) == 'completed':
            logger.info(f"å®éªŒ {config_name} å·²å®Œæˆï¼Œè·³è¿‡ã€‚ä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°è¿è¡Œã€‚")
            return True
        
        logger.info(f"å¼€å§‹è¿è¡Œå®éªŒ: {config_name}, è®­ç»ƒè½®æ•°: {epochs}")
        self.experiment_status[config_name] = 'running'
        self.save_experiment_status()
        
        try:
            # æ„é€ å‘½ä»¤
            cmd = [
                'python', 'training_variants.py',
                '--config', config_name,
                '--epochs', str(epochs)
            ]
            
            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # è¿è¡Œå®éªŒ - ä¿®å¤tqdmæ˜¾ç¤ºé—®é¢˜
            start_time = time.time()
            
            # ä½¿ç”¨å®æ—¶è¾“å‡ºï¼Œä¿æŒtqdmå·¥ä½œ
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1  # è¡Œç¼“å†²
            )
            
            # ä¿å­˜æ—¥å¿—å¹¶å®æ—¶æ˜¾ç¤º
            log_file = os.path.join(self.results_dir, f"{config_name}_training.log")
            with open(log_file, 'w', encoding='utf-8') as f:
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        print(output.strip())  # å®æ—¶æ˜¾ç¤º
                        f.write(output)
                        f.flush()
            
            process.wait()
            end_time = time.time()
            
            if process.returncode == 0:
                logger.info(f"å®éªŒ {config_name} æˆåŠŸå®Œæˆï¼Œç”¨æ—¶: {end_time - start_time:.2f}ç§’")
                self.experiment_status[config_name] = 'completed'
                self.save_experiment_status()
                return True
            else:
                logger.error(f"å®éªŒ {config_name} å¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                self.experiment_status[config_name] = 'failed'
                self.save_experiment_status()
                return False
                
        except Exception as e:
            logger.error(f"è¿è¡Œå®éªŒ {config_name} æ—¶å‡ºé”™: {e}")
            self.experiment_status[config_name] = 'failed'
            self.save_experiment_status()
            return False
    
    def run_sequential_experiments(self, epochs: int = 50, start_from: str = None, force_restart: bool = False):
        """æŒ‰åºåˆ—è¿è¡Œå®éªŒ"""
        logger.info("å¼€å§‹æŒ‰åºåˆ—è¿è¡Œå®éªŒ...")
        
        start_idx = 0
        if start_from:
            for i, (config_name, _, _) in enumerate(self.experiment_configs):
                if config_name == start_from:
                    start_idx = i
                    break
        
        results = {}
        for i, (config_name, description, loss_type) in enumerate(self.experiment_configs[start_idx:], start_idx):
            logger.info(f"\n{'='*60}")
            logger.info(f"å®éªŒ {i+1}/{len(self.experiment_configs)}: {description}")
            logger.info(f"é…ç½®: {config_name}, æŸå¤±ç±»å‹: {loss_type}")
            logger.info(f"{'='*60}")
            
            success = self.run_single_experiment(config_name, epochs, force_restart)
            results[config_name] = success
            
            if success:
                logger.info(f"âœ“ å®éªŒ {config_name} å®Œæˆ")
                # è¿è¡Œå¿«é€Ÿåˆ†æ
                self.quick_analysis_single(config_name)
            else:
                logger.error(f"âœ— å®éªŒ {config_name} å¤±è´¥")
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                try:
                    response = input(f"å®éªŒ {config_name} å¤±è´¥ã€‚æ˜¯å¦ç»§ç»­ä¸‹ä¸€ä¸ªå®éªŒï¼Ÿ(y/n): ")
                    if response.lower() != 'y':
                        logger.info("ç”¨æˆ·é€‰æ‹©åœæ­¢å®éªŒ")
                        break
                except KeyboardInterrupt:
                    logger.info("ç”¨æˆ·ä¸­æ–­å®éªŒ")
                    break
            
            # çŸ­æš‚ä¼‘æ¯
            if i < len(self.experiment_configs) - 1:
                logger.info("ç­‰å¾…10ç§’åç»§ç»­ä¸‹ä¸€ä¸ªå®éªŒ...")
                time.sleep(10)
        
        # è¿è¡Œç»¼åˆåˆ†æ
        logger.info("\nå¼€å§‹ç»¼åˆåˆ†æ...")
        self.comprehensive_analysis()
        
        return results
    
    def quick_test_all(self, epochs: int = 10):
        """å¿«é€Ÿæµ‹è¯•æ‰€æœ‰é…ç½®ï¼ˆç”¨äºéªŒè¯è„šæœ¬æ­£ç¡®æ€§ï¼‰"""
        logger.info(f"å¿«é€Ÿæµ‹è¯•æ‰€æœ‰é…ç½®ï¼Œæ¯ä¸ªé…ç½®è¿è¡Œ {epochs} è½®...")
        
        results = {}
        for config_name, description, _ in self.experiment_configs:
            logger.info(f"å¿«é€Ÿæµ‹è¯•: {config_name} - {description}")
            success = self.run_single_experiment(config_name, epochs)
            results[config_name] = success
            
            if not success:
                logger.error(f"å¿«é€Ÿæµ‹è¯•å¤±è´¥: {config_name}")
        
        return results
    
    def quick_analysis_single(self, config_name: str):
        """å•ä¸ªå®éªŒçš„å¿«é€Ÿåˆ†æ"""
        try:
            # æŸ¥æ‰¾å®éªŒç»“æœç›®å½•
            output_pattern = os.path.join(self.output_base, f"2-{config_name.replace('_', '-')}-*")
            output_dirs = glob.glob(output_pattern)
            
            if not output_dirs:
                output_pattern = os.path.join(self.output_base, f"2-*{config_name}*")
                output_dirs = glob.glob(output_pattern)
            
            if output_dirs:
                output_dir = output_dirs[0]
                
                # æŸ¥æ‰¾å®éªŒæ‘˜è¦
                summary_files = glob.glob(os.path.join(output_dir, "experiment_summary_*.json"))
                if summary_files:
                    with open(summary_files[0], 'r') as f:
                        summary = json.load(f)
                    
                    logger.info(f"ğŸ“Š {config_name} å®éªŒç»“æœ:")
                    logger.info(f"   æœ€ä½³éªŒè¯æŸå¤±: {summary['best_val_loss']:.6f}")
                    logger.info(f"   è®­ç»ƒè½®æ•°: {summary['training_config']['num_epochs']}")
                    logger.info(f"   æŸå¤±ç»„åˆ: {summary['loss_config']['loss_combination']}")
            
        except Exception as e:
            logger.warning(f"å¿«é€Ÿåˆ†æå¤±è´¥ {config_name}: {e}")
    
    def collect_all_results(self) -> pd.DataFrame:
        """æ”¶é›†æ‰€æœ‰å®éªŒç»“æœ"""
        logger.info("æ”¶é›†æ‰€æœ‰å®éªŒç»“æœ...")
        
        results_data = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å®éªŒè¾“å‡ºç›®å½•
        output_pattern = os.path.join(self.output_base, "2-*_outputs_lt135km")
        output_dirs = glob.glob(output_pattern)
        
        for output_dir in output_dirs:
            experiment_name = os.path.basename(output_dir).replace('2-', '').replace('_outputs_lt135km', '')
            
            # æŸ¥æ‰¾å®éªŒæ‘˜è¦æ–‡ä»¶
            summary_files = glob.glob(os.path.join(output_dir, "experiment_summary_*_lt135km.json"))
            
            if summary_files:
                try:
                    with open(summary_files[0], 'r') as f:
                        summary = json.load(f)
                    
                    # æå–å…³é”®æŒ‡æ ‡
                    result_entry = {
                        'experiment': experiment_name,
                        'loss_combination': summary['loss_config']['loss_combination'],
                        'use_tid_weights': summary['loss_config']['use_tid_weights'],
                        'pixel_weight': summary['loss_config']['pixel_loss_weight'],
                        'gradient_weight': summary['loss_config']['gradient_loss_weight'],
                        'spectral_weight': summary['loss_config']['spectral_loss_weight'],
                        'best_val_loss': summary['best_val_loss'],
                        'total_params': summary['total_params'],
                        'num_epochs': summary['training_config']['num_epochs'],
                        'batch_size': summary['training_config']['batch_size'],
                        'learning_rate': summary['training_config']['learning_rate']
                    }
                    
                    results_data.append(result_entry)
                    logger.info(f"æ”¶é›†åˆ°å®éªŒç»“æœ: {experiment_name}")
                    
                except Exception as e:
                    logger.warning(f"è¯»å–å®éªŒæ‘˜è¦å¤±è´¥ {summary_files[0]}: {e}")
        
        if results_data:
            df = pd.DataFrame(results_data)
            return df
        else:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
            return pd.DataFrame()
    
    def create_comparison_plots(self, results_df: pd.DataFrame):
        """åˆ›å»ºå¯¹æ¯”åˆ†æå›¾è¡¨"""
        if results_df.empty:
            logger.warning("æ²¡æœ‰ç»“æœæ•°æ®ï¼Œè·³è¿‡å›¾è¡¨åˆ›å»º")
            return
        
        logger.info("åˆ›å»ºå¯¹æ¯”åˆ†æå›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æœ€ä½³éªŒè¯æŸå¤±å¯¹æ¯”
        sorted_df = results_df.sort_values('best_val_loss')
        ax1 = axes[0, 0]
        bars = ax1.bar(range(len(sorted_df)), sorted_df['best_val_loss'])
        ax1.set_xlabel('å®éªŒé…ç½®')
        ax1.set_ylabel('æœ€ä½³éªŒè¯æŸå¤±')
        ax1.set_title('å„å®éªŒé…ç½®çš„æœ€ä½³éªŒè¯æŸå¤±å¯¹æ¯”')
        ax1.set_xticks(range(len(sorted_df)))
        ax1.set_xticklabels(sorted_df['experiment'], rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.4f}', ha='center', va='bottom')
        
        # 2. TIDæƒé‡ä½¿ç”¨æ•ˆæœå¯¹æ¯”
        ax2 = axes[0, 1]
        if 'use_tid_weights' in results_df.columns:
            tid_comparison = results_df.groupby('use_tid_weights')['best_val_loss'].agg(['mean', 'std', 'count']).reset_index()
            
            if len(tid_comparison) > 1:
                tid_labels = ['ä¸ä½¿ç”¨TIDæƒé‡', 'ä½¿ç”¨TIDæƒé‡']
                x_pos = [0, 1]
                bars = ax2.bar(x_pos, tid_comparison['mean'], yerr=tid_comparison['std'], 
                              capsize=5, alpha=0.7, color=['lightcoral', 'lightblue'])
                ax2.set_xlabel('TIDæƒé‡ä½¿ç”¨æƒ…å†µ')
                ax2.set_ylabel('å¹³å‡æœ€ä½³éªŒè¯æŸå¤±')
                ax2.set_title('TIDæƒé‡ä½¿ç”¨æ•ˆæœå¯¹æ¯”')
                ax2.set_xticks(x_pos)
                ax2.set_xticklabels(tid_labels)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for i, bar in enumerate(bars):
                    height = bar.get_height()
                    ax2.text(bar.get_x() + bar.get_width()/2., height,
                            f'{height:.4f}', ha='center', va='bottom')
        
        # 3. è®­ç»ƒæ•ˆç‡åˆ†æ
        ax3 = axes[1, 0]
        scatter = ax3.scatter(results_df['num_epochs'], results_df['best_val_loss'], 
                            s=100, alpha=0.7, c=results_df.index, cmap='tab10')
        ax3.set_xlabel('è®­ç»ƒè½®æ•°')
        ax3.set_ylabel('æœ€ä½³éªŒè¯æŸå¤±')
        ax3.set_title('è®­ç»ƒæ•ˆç‡åˆ†æ')
        
        # æ·»åŠ å®éªŒåç§°æ³¨é‡Š
        for i, row in results_df.iterrows():
            ax3.annotate(row['experiment'], (row['num_epochs'], row['best_val_loss']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # 4. æŸå¤±å‡½æ•°ç»„åˆæ•ˆæœ
        ax4 = axes[1, 1]
        loss_comparison = results_df.groupby('loss_combination')['best_val_loss'].mean().sort_values()
        bars = ax4.bar(range(len(loss_comparison)), loss_comparison.values)
        ax4.set_xlabel('æŸå¤±å‡½æ•°ç»„åˆ')
        ax4.set_ylabel('å¹³å‡æœ€ä½³éªŒè¯æŸå¤±')
        ax4.set_title('ä¸åŒæŸå¤±å‡½æ•°ç»„åˆæ•ˆæœå¯¹æ¯”')
        ax4.set_xticks(range(len(loss_comparison)))
        ax4.set_xticklabels(loss_comparison.index, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(self.analysis_dir, 'experiment_comparison.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
    
    def generate_analysis_report(self, results_df: pd.DataFrame) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report_lines = []
        report_lines.append("# æµ·åº•åœ°å½¢ç²¾åŒ–å®éªŒ - æŸå¤±å‡½æ•°å¯¹æ¯”åˆ†ææŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        if results_df.empty:
            report_lines.append("## é”™è¯¯: æœªæ‰¾åˆ°å®éªŒç»“æœæ•°æ®")
            return "\n".join(report_lines)
        
        # 1. å®éªŒæ¦‚å†µ
        report_lines.append("## 1. å®éªŒæ¦‚å†µ")
        report_lines.append(f"- æ€»å®éªŒæ•°é‡: {len(results_df)}")
        report_lines.append(f"- å®éªŒé…ç½®: {', '.join(results_df['experiment'].tolist())}")
        report_lines.append("")
        
        # 2. æœ€ä½³ç»“æœ
        best_result = results_df.loc[results_df['best_val_loss'].idxmin()]
        report_lines.append("## 2. æœ€ä½³å®éªŒç»“æœ")
        report_lines.append(f"- **æœ€ä½³é…ç½®**: {best_result['experiment']}")
        report_lines.append(f"- **æœ€ä½³éªŒè¯æŸå¤±**: {best_result['best_val_loss']:.6f}")
        report_lines.append(f"- **æŸå¤±å‡½æ•°ç»„åˆ**: {best_result['loss_combination']}")
        report_lines.append(f"- **ä½¿ç”¨TIDæƒé‡**: {'æ˜¯' if best_result['use_tid_weights'] else 'å¦'}")
        report_lines.append(f"- **è®­ç»ƒè½®æ•°**: {best_result['num_epochs']}")
        report_lines.append("")
        
        # 3. å®éªŒæ’å
        report_lines.append("## 3. å®éªŒæ•ˆæœæ’å")
        sorted_results = results_df.sort_values('best_val_loss')
        for i, (_, row) in enumerate(sorted_results.iterrows(), 1):
            report_lines.append(f"{i}. **{row['experiment']}** - æŸå¤±: {row['best_val_loss']:.6f} ({row['loss_combination']})")
        report_lines.append("")
        
        # 4. å…³é”®å‘ç°
        report_lines.append("## 4. å…³é”®å‘ç°")
        
        # TIDæƒé‡æ•ˆæœåˆ†æ
        if len(results_df['use_tid_weights'].unique()) > 1:
            tid_true = results_df[results_df['use_tid_weights'] == True]['best_val_loss'].mean()
            tid_false = results_df[results_df['use_tid_weights'] == False]['best_val_loss'].mean()
            
            if not np.isnan(tid_true) and not np.isnan(tid_false):
                improvement = (tid_false - tid_true) / tid_false * 100
                report_lines.append("### 4.1 TIDæƒé‡æ•ˆæœ")
                report_lines.append(f"- ä½¿ç”¨TIDæƒé‡å¹³å‡æŸå¤±: {tid_true:.6f}")
                report_lines.append(f"- ä¸ä½¿ç”¨TIDæƒé‡å¹³å‡æŸå¤±: {tid_false:.6f}")
                report_lines.append(f"- æ”¹å–„ç¨‹åº¦: {improvement:.2f}%")
                report_lines.append("")
        
        # æŸå¤±å‡½æ•°ç»„åˆæ•ˆæœ
        report_lines.append("### 4.2 æŸå¤±å‡½æ•°ç»„åˆæ•ˆæœ")
        loss_comparison = results_df.groupby('loss_combination')['best_val_loss'].agg(['mean', 'count']).sort_values('mean')
        for loss_type, stats in loss_comparison.iterrows():
            report_lines.append(f"- **{loss_type}**: å¹³å‡æŸå¤± {stats['mean']:.6f} (å®éªŒæ•°: {stats['count']})")
        report_lines.append("")
        
        # 5. å»ºè®®
        report_lines.append("## 5. ä¸‹ä¸€æ­¥å»ºè®®")
        report_lines.append("åŸºäºå½“å‰å®éªŒç»“æœçš„å»ºè®®:")
        
        if best_result['use_tid_weights']:
            report_lines.append("- âœ“ ç»§ç»­ä½¿ç”¨TIDæƒé‡ï¼Œç¡®è®¤æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½")
        
        if best_result['gradient_weight'] > 0:
            report_lines.append("- âœ“ æ¢¯åº¦æŸå¤±è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®åœ¨åç»­å®éªŒä¸­ä¿ç•™")
        
        if best_result['spectral_weight'] > 0:
            report_lines.append("- âœ“ é¢‘è°±æŸå¤±æœ‰æ•ˆï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æƒé‡é…ç½®")
        
        report_lines.append("")
        report_lines.append("## 6. è¯¦ç»†ç»“æœè¡¨æ ¼")
        report_lines.append("```")
        report_lines.append(results_df.round(6).to_string(index=False))
        report_lines.append("```")
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.analysis_dir, 'experiment_analysis_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_content
    
    def comprehensive_analysis(self):
        """ç»¼åˆåˆ†æ"""
        logger.info("å¼€å§‹ç»¼åˆåˆ†æ...")
        
        # æ”¶é›†ç»“æœ
        results_df = self.collect_all_results()
        
        if results_df.empty:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å®éªŒç»“æœï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return
        
        # ä¿å­˜ç»“æœæ•°æ®
        results_csv_path = os.path.join(self.analysis_dir, 'experiment_results.csv')
        results_df.to_csv(results_csv_path, index=False)
        logger.info(f"ç»“æœæ•°æ®å·²ä¿å­˜åˆ°: {results_csv_path}")
        
        # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
        self.create_comparison_plots(results_df)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = self.generate_analysis_report(results_df)
        
        # æ˜¾ç¤ºæ‘˜è¦
        logger.info("\n" + "="*60)
        logger.info("å®éªŒæ‘˜è¦:")
        logger.info("="*60)
        
        if not results_df.empty:
            best_result = results_df.loc[results_df['best_val_loss'].idxmin()]
            logger.info(f"ğŸ† æœ€ä½³é…ç½®: {best_result['experiment']}")
            logger.info(f"ğŸ“Š æœ€ä½³æŸå¤±: {best_result['best_val_loss']:.6f}")
            logger.info(f"ğŸ”§ æŸå¤±ç»„åˆ: {best_result['loss_combination']}")
            logger.info(f"âš–ï¸  ä½¿ç”¨TIDæƒé‡: {'æ˜¯' if best_result['use_tid_weights'] else 'å¦'}")
        
        logger.info("="*60)
        logger.info("ç»¼åˆåˆ†æå®Œæˆï¼")
    
    def show_status(self):
        """æ˜¾ç¤ºå®éªŒçŠ¶æ€"""
        logger.info("å½“å‰å®éªŒçŠ¶æ€:")
        logger.info("-" * 40)
        
        for config_name, description, _ in self.experiment_configs:
            status = self.experiment_status.get(config_name, 'not_started')
            status_symbol = {
                'not_started': 'âšª',
                'running': 'ğŸŸ¡',
                'completed': 'ğŸŸ¢',
                'failed': 'ğŸ”´'
            }.get(status, 'â“')
            
            logger.info(f"{status_symbol} {config_name}: {description} - {status}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®éªŒç®¡ç†å™¨ - ç³»ç»ŸåŒ–çš„æŸå¤±å‡½æ•°å¯¹æ¯”å®éªŒ')
    parser.add_argument('--mode', type=str, default='sequential',
                        choices=['sequential', 'analyze', 'quick_test', 'status'],
                        help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--start_from', type=str, help='ä»æŒ‡å®šé…ç½®å¼€å§‹è¿è¡Œ')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°è¿è¡Œå·²å®Œæˆçš„å®éªŒ')
    
    args = parser.parse_args()
    
    manager = ExperimentManager()
    
    if args.mode == 'sequential':
        # æŒ‰åºåˆ—è¿è¡Œå®éªŒ
        logger.info(f"æŒ‰åºåˆ—è¿è¡Œå®éªŒï¼Œæ¯ä¸ªé…ç½® {args.epochs} è½®è®­ç»ƒ")
        if args.start_from:
            logger.info(f"ä»é…ç½® {args.start_from} å¼€å§‹")
        
        manager.run_sequential_experiments(
            epochs=args.epochs, 
            start_from=args.start_from,
            force_restart=args.force
        )
        
    elif args.mode == 'analyze':
        # åªè¿›è¡Œåˆ†æ
        logger.info("åˆ†æå·²æœ‰å®éªŒç»“æœ...")
        manager.comprehensive_analysis()
        
    elif args.mode == 'quick_test':
        # å¿«é€Ÿæµ‹è¯•
        logger.info(f"å¿«é€Ÿæµ‹è¯•æ‰€æœ‰é…ç½®ï¼Œæ¯ä¸ªé…ç½® {args.epochs} è½®è®­ç»ƒ")
        manager.quick_test_all(epochs=args.epochs)
        
    elif args.mode == 'status':
        # æ˜¾ç¤ºçŠ¶æ€
        manager.show_status()
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()