#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼ºè®­ç»ƒè„šæœ¬ - åŒ…å«å®Œæ•´å¯è§†åŒ–åŠŸèƒ½

è¿™ä¸ªè„šæœ¬åœ¨åŸæœ‰è®­ç»ƒåŸºç¡€ä¸Šæ·»åŠ äº†ï¼š
1. å®Œæ•´çš„è®­ç»ƒå†å²å¯è§†åŒ–
2. é¢„æµ‹ç¤ºä¾‹å›¾
3. æ•£ç‚¹å›¾å¯¹æ¯”
4. è¯¦ç»†çš„å®éªŒæŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
python enhanced_training_with_viz.py --config baseline --epochs 20

ä½œè€…: å‘¨ç‘å®¸
æ—¥æœŸ: 2025-05-25
"""

import argparse
import sys
import os
import numpy
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader
import json
import logging
from datetime import datetime

# å°†çˆ¶ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_training_multi_loss import EnhancedTrainingConfig
import enhanced_training_multi_loss


def convert_to_python_types(data):
    if isinstance(data, dict):
        return {key: convert_to_python_types(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [convert_to_python_types(item) for item in data]
    elif isinstance(data, numpy.float32) or isinstance(data, numpy.float64): # ä½ ä¹Ÿå¯ä»¥ç”¨ numpy.floating
        return float(data)
    elif isinstance(data, numpy.int32) or isinstance(data, numpy.int64): # ä½ ä¹Ÿå¯ä»¥ç”¨ numpy.integer
        return int(data)
    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»– numpy ç±»å‹çš„è½¬æ¢
    return data


logger = logging.getLogger('enhanced_training_with_viz')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def create_training_visualization(trainer, config, output_dir):
    """åˆ›å»ºå®Œæ•´çš„è®­ç»ƒå¯è§†åŒ–"""
    logger.info("åˆ›å»ºè®­ç»ƒå†å²å¯è§†åŒ–...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    epochs = range(1, len(trainer.history['train_loss_total']) + 1)
    
    # æ€»æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, trainer.history['train_loss_total'], 'b-', label='è®­ç»ƒæ€»æŸå¤±', alpha=0.7)
    axes[0, 0].plot(epochs, trainer.history['val_loss_total'], 'r-', label='éªŒè¯æ€»æŸå¤±', alpha=0.7)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æ€»æŸå¤±')
    axes[0, 0].set_title('æ€»æŸå¤±å˜åŒ–')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # åƒç´ æŸå¤±æ›²çº¿
    axes[0, 1].plot(epochs, trainer.history['train_loss_pixel'], 'b-', label='è®­ç»ƒåƒç´ æŸå¤±', alpha=0.7)
    axes[0, 1].plot(epochs, trainer.history['val_loss_pixel'], 'r-', label='éªŒè¯åƒç´ æŸå¤±', alpha=0.7)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('åƒç´ æŸå¤±')
    axes[0, 1].set_title('åƒç´ æŸå¤±å˜åŒ–')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # æ¢¯åº¦æŸå¤±æ›²çº¿
    axes[0, 2].plot(epochs, trainer.history['train_loss_gradient'], 'b-', label='è®­ç»ƒæ¢¯åº¦æŸå¤±', alpha=0.7)
    axes[0, 2].plot(epochs, trainer.history['val_loss_gradient'], 'r-', label='éªŒè¯æ¢¯åº¦æŸå¤±', alpha=0.7)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('æ¢¯åº¦æŸå¤±')
    axes[0, 2].set_title('æ¢¯åº¦æŸå¤±å˜åŒ–')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # é¢‘è°±æŸå¤±æ›²çº¿
    axes[1, 0].plot(epochs, trainer.history['train_loss_spectral'], 'b-', label='è®­ç»ƒé¢‘è°±æŸå¤±', alpha=0.7)
    axes[1, 0].plot(epochs, trainer.history['val_loss_spectral'], 'r-', label='éªŒè¯é¢‘è°±æŸå¤±', alpha=0.7)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('é¢‘è°±æŸå¤±')
    axes[1, 0].set_title('é¢‘è°±æŸå¤±å˜åŒ–')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç›¸å…³ç³»æ•°
    axes[1, 1].plot(epochs, trainer.history['val_correlation'], 'g-', label='éªŒè¯ç›¸å…³ç³»æ•°', alpha=0.7)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('ç›¸å…³ç³»æ•°')
    axes[1, 1].set_title('ç›¸å…³ç³»æ•°å˜åŒ–')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡æ›²çº¿
    axes[1, 2].plot(epochs, trainer.history['learning_rates'], 'orange', alpha=0.7)
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('å­¦ä¹ ç‡')
    axes[1, 2].set_title('å­¦ä¹ ç‡å˜åŒ–')
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].set_yscale('log')
    
    plt.suptitle(f'è®­ç»ƒå†å² - {config.LOSS_CONFIG["loss_combination"]} æŸå¤±ç»„åˆ', fontsize=14)
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    plot_path = os.path.join(output_dir, f'training_history_{config.LOSS_CONFIG["loss_combination"]}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"è®­ç»ƒå†å²å›¾ä¿å­˜åˆ°: {plot_path}")


def create_prediction_examples(model, test_loader, config, output_dir, device):
    """åˆ›å»ºé¢„æµ‹ç¤ºä¾‹å›¾"""
    logger.info("åˆ›å»ºé¢„æµ‹ç¤ºä¾‹...")
    
    model.eval()
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•æ•°æ®
    with torch.no_grad():
        for batch_data in test_loader:
            if len(batch_data) == 3:
                features, labels, tid_weights = batch_data
                features = features.to(device)
                labels = labels.to(device)
                tid_weights = tid_weights.to(device)
            else:
                features, labels = batch_data
                features = features.to(device)
                labels = labels.to(device)
                tid_weights = None
            
            predictions = model(features)
            break
    
    # é€‰æ‹©å‡ ä¸ªç¤ºä¾‹è¿›è¡Œå¯è§†åŒ–
    n_examples = min(4, predictions.size(0))
    
    fig, axes = plt.subplots(n_examples, 3, figsize=(15, 5*n_examples))
    if n_examples == 1:
        axes = axes.reshape(1, -1)
    
    for i in range(n_examples):
        pred = predictions[i, 0].cpu().numpy()
        true = labels[i, 0].cpu().numpy()
        error = pred - true
        
        # çœŸå€¼
        im1 = axes[i, 0].imshow(true, cmap='RdBu_r', aspect='auto')
        axes[i, 0].set_title(f'çœŸå€¼ {i+1}')
        plt.colorbar(im1, ax=axes[i, 0])
        
        # é¢„æµ‹
        im2 = axes[i, 1].imshow(pred, cmap='RdBu_r', aspect='auto')
        axes[i, 1].set_title(f'é¢„æµ‹ {i+1}')
        plt.colorbar(im2, ax=axes[i, 1])
        
        # è¯¯å·®
        im3 = axes[i, 2].imshow(error, cmap='RdBu_r', aspect='auto')
        axes[i, 2].set_title(f'è¯¯å·® {i+1}')
        plt.colorbar(im3, ax=axes[i, 2])
    
    plt.suptitle(f'é¢„æµ‹ç¤ºä¾‹ - {config.LOSS_CONFIG["loss_combination"]}', fontsize=14)
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    examples_path = os.path.join(output_dir, f'prediction_examples_{config.LOSS_CONFIG["loss_combination"]}.png')
    plt.savefig(examples_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"é¢„æµ‹ç¤ºä¾‹å›¾ä¿å­˜åˆ°: {examples_path}")


def create_scatter_plot(model, test_loader, config, output_dir, device):
    """åˆ›å»ºé¢„æµ‹vsçœŸå€¼æ•£ç‚¹å›¾"""
    logger.info("åˆ›å»ºæ•£ç‚¹å›¾...")
    
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_data in test_loader:
            if len(batch_data) == 3:
                features, labels, _ = batch_data
                features = features.to(device)
                labels = labels.to(device)
            else:
                features, labels = batch_data
                features = features.to(device)
                labels = labels.to(device)
            
            predictions = model(features)
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(labels.cpu().numpy())
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    pred_flat = predictions.flatten()
    target_flat = targets.flatten()
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    mse = np.mean((pred_flat - target_flat) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(pred_flat - target_flat))
    correlation = np.corrcoef(pred_flat, target_flat)[0, 1]
    
    # åˆ›å»ºæ•£ç‚¹å›¾
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # é‡‡æ ·æ•°æ®ç‚¹ï¼ˆé¿å…è¿‡å¤šæ•°æ®ç‚¹ï¼‰
    n_samples = min(50000, len(pred_flat))
    indices = np.random.choice(len(pred_flat), n_samples, replace=False)
    pred_sample = pred_flat[indices]
    target_sample = target_flat[indices]
    
    ax.scatter(target_sample, pred_sample, alpha=0.3, s=1, c='blue')
    
    # ç»˜åˆ¶ç†æƒ³çº¿
    min_val = min(target_sample.min(), pred_sample.min())
    max_val = max(target_sample.max(), pred_sample.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹')
    
    ax.set_xlabel('çœŸå€¼')
    ax.set_ylabel('é¢„æµ‹å€¼')
    ax.set_title(f'é¢„æµ‹ vs çœŸå€¼æ•£ç‚¹å›¾\n'
                f'RMSE: {rmse:.4f}, MAE: {mae:.4f}, ç›¸å…³ç³»æ•°: {correlation:.4f}')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # è®¾ç½®ç›¸åŒçš„åæ ‡è½´èŒƒå›´
    ax.set_xlim(min_val, max_val)
    ax.set_ylim(min_val, max_val)
    ax.set_aspect('equal')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    scatter_path = os.path.join(output_dir, f'scatter_plot_{config.LOSS_CONFIG["loss_combination"]}.png')
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"æ•£ç‚¹å›¾ä¿å­˜åˆ°: {scatter_path}")
    
    return {'rmse': rmse, 'mae': mae, 'correlation': correlation}


def create_detailed_report(config, trainer, test_metrics, output_dir, total_params):
    """åˆ›å»ºè¯¦ç»†çš„å®éªŒæŠ¥å‘Š"""
    logger.info("ç”Ÿæˆè¯¦ç»†å®éªŒæŠ¥å‘Š...")
    
    report_lines = []
    report_lines.append(f"# æµ·åº•åœ°å½¢ç²¾åŒ–å®éªŒæŠ¥å‘Š")
    report_lines.append(f"## é…ç½®: {config.LOSS_CONFIG['loss_combination']}")
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. å®éªŒé…ç½®
    report_lines.append("## 1. å®éªŒé…ç½®")
    report_lines.append(f"- **æŸå¤±å‡½æ•°ç»„åˆ**: {config.LOSS_CONFIG['loss_combination']}")
    report_lines.append(f"- **ä½¿ç”¨TIDæƒé‡**: {'æ˜¯' if config.LOSS_CONFIG['use_tid_weights'] else 'å¦'}")
    report_lines.append(f"- **åƒç´ æŸå¤±æƒé‡**: {config.LOSS_CONFIG['pixel_loss_weight']}")
    report_lines.append(f"- **æ¢¯åº¦æŸå¤±æƒé‡**: {config.LOSS_CONFIG['gradient_loss_weight']}")
    report_lines.append(f"- **é¢‘è°±æŸå¤±æƒé‡**: {config.LOSS_CONFIG['spectral_loss_weight']}")
    report_lines.append(f"- **æ‰¹æ¬¡å¤§å°**: {config.BATCH_SIZE}")
    report_lines.append(f"- **å­¦ä¹ ç‡**: {config.LEARNING_RATE}")
    report_lines.append(f"- **æƒé‡è¡°å‡**: {config.WEIGHT_DECAY}")
    report_lines.append("")
    
    # 2. æ¨¡å‹ä¿¡æ¯
    report_lines.append("## 2. æ¨¡å‹ä¿¡æ¯")
    report_lines.append(f"- **æ€»å‚æ•°æ•°é‡**: {total_params:,}")
    report_lines.append(f"- **è¾“å…¥é€šé“æ•°**: {config.IN_CHANNELS}")
    report_lines.append(f"- **è¾“å‡ºé€šé“æ•°**: {config.OUT_CHANNELS}")
    report_lines.append(f"- **åˆå§‹ç‰¹å¾æ•°**: {config.INIT_FEATURES}")
    report_lines.append("")
    
    # 3. è®­ç»ƒç»“æœ
    report_lines.append("## 3. è®­ç»ƒç»“æœ")
    report_lines.append(f"- **è®­ç»ƒè½®æ•°**: {len(trainer.history['train_loss_total'])}")
    report_lines.append(f"- **æœ€ä½³éªŒè¯æŸå¤±**: {trainer.best_val_loss:.6f}")
    report_lines.append(f"- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {trainer.history['train_loss_total'][-1]:.6f}")
    report_lines.append(f"- **æœ€ç»ˆéªŒè¯ç›¸å…³ç³»æ•°**: {trainer.history['val_correlation'][-1]:.6f}")
    report_lines.append("")
    
    # 4. æµ‹è¯•é›†è¯„ä¼°
    report_lines.append("## 4. æµ‹è¯•é›†è¯„ä¼°")
    report_lines.append(f"- **RMSE**: {test_metrics['rmse']:.6f}")
    report_lines.append(f"- **MAE**: {test_metrics['mae']:.6f}")
    report_lines.append(f"- **ç›¸å…³ç³»æ•°**: {test_metrics['correlation']:.6f}")
    report_lines.append("")
    
    # 5. æŸå¤±å‡½æ•°åˆ†æ
    if len(trainer.history['train_loss_total']) > 0:
        report_lines.append("## 5. æŸå¤±å‡½æ•°åˆ†æ")
        final_pixel_loss = trainer.history['val_loss_pixel'][-1]
        final_gradient_loss = trainer.history['val_loss_gradient'][-1]
        final_spectral_loss = trainer.history['val_loss_spectral'][-1]
        
        report_lines.append(f"- **æœ€ç»ˆåƒç´ æŸå¤±**: {final_pixel_loss:.6f}")
        report_lines.append(f"- **æœ€ç»ˆæ¢¯åº¦æŸå¤±**: {final_gradient_loss:.6f}")
        report_lines.append(f"- **æœ€ç»ˆé¢‘è°±æŸå¤±**: {final_spectral_loss:.6f}")
        report_lines.append("")
    
    # 6. è¾“å‡ºæ–‡ä»¶
    report_lines.append("## 6. è¾“å‡ºæ–‡ä»¶")
    report_lines.append(f"- **æ¨¡å‹æƒé‡**: best_model_enhanced_multi_loss_{config.LOSS_CONFIG['loss_combination']}_lt135km.pth")
    report_lines.append(f"- **è®­ç»ƒå†å²å›¾**: training_history_{config.LOSS_CONFIG['loss_combination']}.png")
    report_lines.append(f"- **é¢„æµ‹ç¤ºä¾‹å›¾**: prediction_examples_{config.LOSS_CONFIG['loss_combination']}.png")
    report_lines.append(f"- **æ•£ç‚¹å›¾**: scatter_plot_{config.LOSS_CONFIG['loss_combination']}.png")
    report_lines.append(f"- **å®éªŒæ‘˜è¦**: experiment_summary_{config.LOSS_CONFIG['loss_combination']}_lt135km.json")
    
    report_content = "\n".join(report_lines)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(output_dir, f'detailed_report_{config.LOSS_CONFIG["loss_combination"]}.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"è¯¦ç»†æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")


# é…ç½®ç±»å®šä¹‰ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
class BaselineConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-baseline_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'baseline',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.0,
            'spectral_loss_weight': 0.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': False,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class WeightedMSEConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-weighted_mse_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'weighted_mse',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.0,
            'spectral_loss_weight': 0.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class MSEGradientConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-mse_grad_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'weighted_mse_grad',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.3,
            'spectral_loss_weight': 0.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class MSESpectralConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-mse_fft_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'weighted_mse_fft',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.0,
            'spectral_loss_weight': 1.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }

class AllCombinedConfig(EnhancedTrainingConfig):
    def __init__(self):
        super().__init__()
        self.OUTPUT_DIR = "/mnt/data5/06-Projects/05-unet/output/2-all_combined_outputs_lt135km"
        self.LOSS_CONFIG = {
            'loss_combination': 'all',
            'pixel_loss_weight': 1.0,
            'gradient_loss_weight': 0.3,
            'spectral_loss_weight': 1.0,
            'pixel_loss_type': 'mse',
            'use_tid_weights': True,
            'tid_weight_scale': 1.0,
            'gradient_method': 'sobel',
            'spectral_focus_high_freq': True,
            'high_freq_weight_factor': 2.0,
        }


def get_config(config_name):
    """æ ¹æ®é…ç½®åç§°è·å–é…ç½®å¯¹è±¡"""
    config_map = {
        'baseline': BaselineConfig,
        'weighted_mse': WeightedMSEConfig,
        'mse_grad': MSEGradientConfig,
        'mse_fft': MSESpectralConfig,
        'all_combined': AllCombinedConfig,
    }
    
    if config_name not in config_map:
        available_configs = list(config_map.keys())
        raise ValueError(f"æœªçŸ¥é…ç½®: {config_name}. å¯ç”¨é…ç½®: {available_configs}")
    
    return config_map[config_name]()


def main():
    """ä¸»å‡½æ•° - åŒ…å«å®Œæ•´å¯è§†åŒ–çš„è®­ç»ƒæµç¨‹"""
    parser = argparse.ArgumentParser(description='å¢å¼ºæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ - åŒ…å«å®Œæ•´å¯è§†åŒ–')
    parser.add_argument('--config', type=str, required=True,
                        choices=['baseline', 'weighted_mse', 'mse_grad', 'mse_fft', 'all_combined'],
                        help='é€‰æ‹©è®­ç»ƒé…ç½®')
    parser.add_argument('--epochs', type=int, default=None, help='è¦†ç›–é»˜è®¤çš„è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=None, help='è¦†ç›–é»˜è®¤çš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=None, help='è¦†ç›–é»˜è®¤çš„å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_config(args.config)
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.epochs is not None:
        config.NUM_EPOCHS = args.epochs
    if args.batch_size is not None:
        config.BATCH_SIZE = args.batch_size
    if args.learning_rate is not None:
        config.LEARNING_RATE = args.learning_rate
    
    logger.info(f"ä½¿ç”¨é…ç½®: {args.config}")
    logger.info(f"æŸå¤±å‡½æ•°ç»„åˆ: {config.LOSS_CONFIG['loss_combination']}")
    logger.info(f"è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œå¯è§†åŒ–æµç¨‹
    logger.info("=== å¢å¼ºæµ·åº•åœ°å½¢ç²¾åŒ–è®­ç»ƒ - åŒ…å«å®Œæ•´å¯è§†åŒ– ===")
    logger.info(f"æŸå¤±å‡½æ•°ç»„åˆ: {config.LOSS_CONFIG['loss_combination']}")
    logger.info(f"æ•°æ®ç›®å½•: {config.DATA_DIR}")
    logger.info(f"è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.RANDOM_SEED)
    np.random.seed(config.RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.RANDOM_SEED)
        torch.cuda.manual_seed_all(config.RANDOM_SEED)

    # æ•°æ®å‡†å¤‡
    data_prep = enhanced_training_multi_loss.EnhancedDataPreparation(config)

    try:
        # åŠ è½½æ•°æ®
        features_ds, labels_da, tid_weights_da = data_prep.load_data()

        # æå–æ•°æ®å—
        patches = data_prep.extract_patches(features_ds, labels_da, tid_weights_da)

        if len(patches) == 0:
            logger.error("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„æ•°æ®å—ï¼")
            return

        # åˆ’åˆ†æ•°æ®é›†
        train_patches, val_patches, test_patches = data_prep.split_patches(patches)

        # åˆ›å»ºæ•°æ®é›†
        train_dataset = enhanced_training_multi_loss.EnhancedBathymetryDataset(train_patches, config, is_training=True)
        val_dataset = enhanced_training_multi_loss.EnhancedBathymetryDataset(val_patches, config, is_training=False)
        test_dataset = enhanced_training_multi_loss.EnhancedBathymetryDataset(test_patches, config, is_training=False)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, 
                                num_workers=config.NUM_WORKERS, pin_memory=torch.cuda.is_available(), drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, 
                              num_workers=config.NUM_WORKERS, pin_memory=torch.cuda.is_available())
        test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, 
                                num_workers=config.NUM_WORKERS, pin_memory=torch.cuda.is_available())

        # åˆ›å»ºæ¨¡å‹
        model = enhanced_training_multi_loss.OptimizedUNet(
            in_channels=config.IN_CHANNELS,
            out_channels=config.OUT_CHANNELS,
            init_features=config.INIT_FEATURES
        )

        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")

        # è®­ç»ƒæ¨¡å‹
        trainer = enhanced_training_multi_loss.EnhancedTrainer(model, config)
        trainer.train(train_loader, val_loader)

        # åˆ›å»ºå®Œæ•´çš„å¯è§†åŒ–
        create_training_visualization(trainer, config, config.OUTPUT_DIR)
        
        # åˆ›å»ºé¢„æµ‹ç¤ºä¾‹å›¾
        create_prediction_examples(model, test_loader, config, config.OUTPUT_DIR, device)
        
        # åˆ›å»ºæ•£ç‚¹å›¾å¹¶è·å–æµ‹è¯•æŒ‡æ ‡
        test_metrics = create_scatter_plot(model, test_loader, config, config.OUTPUT_DIR, device)
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        create_detailed_report(config, trainer, test_metrics, config.OUTPUT_DIR, total_params)

        # ä¿å­˜å®éªŒæ‘˜è¦ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰
        experiment_summary = {
            'experiment_name': f'enhanced_multi_loss_{config.LOSS_CONFIG["loss_combination"]}',
            'loss_config': config.LOSS_CONFIG,
            'model_config': {
                'in_channels': config.IN_CHANNELS,
                'out_channels': config.OUT_CHANNELS,
                'init_features': config.INIT_FEATURES
            },
            'training_config': {
                'batch_size': config.BATCH_SIZE,
                'learning_rate': config.LEARNING_RATE,
                'num_epochs': len(trainer.history['train_loss_total']),
                'early_stopping_patience': config.EARLY_STOPPING_PATIENCE
            },
            'best_val_loss': trainer.best_val_loss,
            'total_params': total_params,
            'trainable_params': total_params,
            'test_metrics': test_metrics
        }

        summary_path = os.path.join(config.OUTPUT_DIR, 
                                  f'experiment_summary_{config.LOSS_CONFIG["loss_combination"]}_lt135km.json')

        experiment_summary_serializable = convert_to_python_types(experiment_summary)

        with open(summary_path, 'w') as f:
            # json.dump(experiment_summary, f, indent=2)
            json.dump(experiment_summary_serializable, f, indent=2)

        logger.info(f"âœ… å¢å¼ºè®­ç»ƒå®Œæˆï¼æŸå¤±ç»„åˆ: {config.LOSS_CONFIG['loss_combination']}")
        logger.info(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {trainer.best_val_loss:.6f}")
        logger.info(f"ğŸ“ˆ æµ‹è¯•RMSE: {test_metrics['rmse']:.6f}")
        logger.info(f"ğŸ“ˆ æµ‹è¯•ç›¸å…³ç³»æ•°: {test_metrics['correlation']:.6f}")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config.OUTPUT_DIR}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        logger.info("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        output_files = [
            f"best_model_enhanced_multi_loss_{config.LOSS_CONFIG['loss_combination']}_lt135km.pth",
            f"experiment_summary_{config.LOSS_CONFIG['loss_combination']}_lt135km.json",
            f"training_history_{config.LOSS_CONFIG['loss_combination']}.png",
            f"prediction_examples_{config.LOSS_CONFIG['loss_combination']}.png",
            f"scatter_plot_{config.LOSS_CONFIG['loss_combination']}.png",
            f"detailed_report_{config.LOSS_CONFIG['loss_combination']}.md"
        ]
        
        for file_name in output_files:
            file_path = os.path.join(config.OUTPUT_DIR, file_name)
            if os.path.exists(file_path):
                logger.info(f"   âœ“ {file_name}")
            else:
                logger.info(f"   âœ— {file_name} (æœªæ‰¾åˆ°)")

    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()